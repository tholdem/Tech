\documentclass[12pt,class=article,crop=false]{standalone} 
\input{../preamble.tex}

\begin{document}
\subsection{Invariant EKF}

\begin{defn}
Let $ G$ be a Lie group and $ f_u: G \to TG$ be a input-dependent vector field. Then $ f_u$ is  \allbold{group-affine} if it satisfies 
\begin{align*}
	f_u(ab) = f_u(a) b + a f_u(b) - a f_u( Id) b  \in T_{ab}G.
\end{align*}
\end{defn}
Group-affine is the weakest assumption one needs on the dynamics to have trajectory-independent error with log-linear error dynamics.

Given two state trajectories $ X,\wh{ X}$ that follows the same deterministic dynamics, let $ \eta^{L} := X^{-1}\wh{ X}$ and $ \eta^R := \wh{ X} X^{-1} $ be their left and right errors. Then the first theorem says that $ f_u$ is group affine iff $ \eta^L$ is state trajectory-independent iff $ \eta^R$ is state trajectory-independent.

\begin{thm}
Let $ i=L,R$. Let  $ \xi^i_0 = (\log(\eta_0^{i}))^\vee$. Define $ A^{i}$ as
\begin{align*}
	g_u( \exp( \xi)) = \left( A^{i} \xi \right)^\vee + O(\norm{ \xi}^2). 
\end{align*}
If $ \xi^{i}$ is defined $ \ \forall \ t>0$ by the linear diffeq in $ \rr^{n}$ :
\begin{align*}
	\frac{d}{dt} \xi^{i} = A^{i} \xi^{i} .
\end{align*}
Then for the true nonlinear error $ \eta^{i}$, we have $ \eta^{i} = \exp( \xi^{i}) \ \forall \ t \geq 0$.
\end{thm}

Let $ \eta = X^{-1} \wh{ X} $ be the left-invariant error. In EKF, $ \wh{ X}$ represents estimated state and $ X$ is the true state, where they share the same deterministic dynamics and only differ in initial state. In the context of uncertainty propagation, $ X$ is modeled by a stochastic process governed by an SDE, and $ \wh{ X}$ is the propagated mean.

When the dynamics $ f_u$ is group-affine, the evolution of the error $ \eta$ satisfies
\begin{align*}
	\frac{d}{dt} \eta = g_u (\eta)
\end{align*}
where $ g_u$ satisfies $ g_u(\eta) = f_u(\eta) - f_u( Id) \eta$. It turns out that $ g_u$ can be converted to a linear dynamic under the exponential coordinates. Let  $ (A \xi) ^\wedge$ be the first-order approximation of  $ g_u (\exp (\xi))$. Then we have $ \eta = \exp( \xi)$ for all time where
\begin{align*}
	\frac{d}{dt} \xi = A \xi .
\end{align*}
This means that the error $ \eta$ is trajectory-independent and can be linearized in the Lie algebra without approximation. 

Note that if $ f_u$ is affine (\emph{i.e.} $ f_u(x) = f_0 + Ax + Bu$), we know that the error has the linear dynamics  $ \dot{e} = A e$, independent of the state-trajectory. This is a generalization of that.

So in a stochastic setting, if the drift of SDE is group-affine, then no matter how state deviates from the mean, the drift error is trajectory-independent and can be propagated using the same ODE in the Lie algebra. If the drift is not group-affine, then using the drift equation (Ricatti?) along the mean trajectory should give additional error.

Euler-Poincare Equation:
\begin{align*}
	\mathbb{I} \dot{\xi} = \ad_{ \xi}^* ( \mathbb{I} \xi) + u 
\end{align*}
This can be linearized and then used for trajectory tracking. This requires already having a nominal trajectory, but in covariance steering we have to produce the nominal.

\subsection{Uncertainty Propagation on Lie Groups}

In the Euclidean case, an SDE $ dx = h(x,t) dt + H(t) dW$ has the following evolution of the first two moments:
 \begin{align*}
	\begin{cases}
		 \dot{\mu} = \left\langle h \right\rangle ,\\
		 \dot{\Sigma} = \left\langle h (x-\mu)^{T} + (x-\mu) h^{T} \right\rangle + HH^{T} .
	\end{cases}
\end{align*}
We can define SDE on $ G$ in three different ways. The first is the McKean-Gangolli injection:
 \begin{align*}
	g(t+ \d t) = g(t) \exp( h(g(t),t) \d t + H(g(t+k\d t), t+k\d t) \d W) .
\end{align*}
If $ k=0$, then the SDE is Ito's. If  $ k=\frac{1}{2}$, then it is Stratonovich's, which we distinguish by using $ \textcircled{s}$.

Stratonovich is natural and obeys the geometry and chain rule. However, it is extremely difficult to evaluate the expectation TODO. Thus, we rely on Ito's to compute expectations and then convert to Stratonovich's if needed. The conversion is
\begin{align*}
	h = h^s + \frac{1}{2} E_i^r\left( H_{kj}^s \right) H_{ij}^s e_k , \qquad H = H^s .
\end{align*}

We shall use the simplified notation
\begin{align*}
	(g^{-1} \d g)^\vee &= h\d t + H\d W\\
	(g^{-1} \d g)^\vee &= h^s \d t + H^s \textcircled{s} \d W .
\end{align*}


The second way is to parametrize group element $ g = g(q)$ and write
 \begin{align*}
	q(t+\d t) = q(t) + (J_r^{-1} \widetilde{ h})(q,t) \d t + (J_r^{-1} \widetilde{ H})(q(t+k\d t),t + k\d t) \d W .
\end{align*}

Using the exponential coordinates $ g(x) = \mu(t) \exp( x)$, under Stratonovich interpretation, to describe the same stochastic process, the drift and diffusion of both ways equal each other. Under Ito's interpretation, there is an additional drift term coming from nonlinearity of exponential map.

\begin{align*}
	\widetilde{ h}(x,t) = h(x,t) + \left( \frac{1}{2} J_r \frac{\partial J_r^{-1}}{\partial x_k} H H^{T} J_r^{-T}  \right) \bigg|_{g=\mu \exp( (x))} e_k,\\
	\widetilde{ H}(x,t) = H(\mu \exp( x),t) .
\end{align*}
Note that if $ H$ does not depend on the state, then Ito's and Stranotovich coincide for $ H$. However,  $ J_r$ could still be different under the two interpretations.

In reality, we are given $ h^s$ and  $ H^s$ TODO, which describe an SDE in the tangent space at each  $ g$. We would have to compute the $ h, H$ or  $ \widetilde{ h}, \widetilde{ H}$ that would yield the same stochastic process.


If $ g$ is not near the identity, then the exponential coordinates might not be convenient to use because of high nonlinearity of exponential map away from the identity and non-bijectiveness of the exponential map. In this case, a moving coordinate frame by re-centering solves the issue. However, a moving frame makes it harder to do covariance steering. We might have to do covariance steering on the error space.

The goal is to derive the error mean and covariance dynamics using the exponential coordinates. We can achieve this via computing the Fokker-Planck equation of 

\begin{defn}
Let $ p$ be a probability density function on an unimodular Lie group $ G$. The  \allbold{group-theoretic mean} $ \mu$ is defined by
\begin{align*}
	\int_G \log ^\vee \left( \mu^{-1} g \right) p(g) \d g = 0 .
\end{align*}
The covariance of $ p$ is defined by
 \begin{align*}
	\Sigma = \int_G \left[ \log ^\vee (\mu^{-1} g) \right] \left[ \log ^\vee (\mu^{-1}g) \right] ^{T} p(g) \d g .
\end{align*}
\end{defn}

\begin{thm}
Let $ x \in \rr^{N}$ be a random variable with concentrated probability distribution around the origin of $ D$. Let its mean, covariance, and probability density function be  $ m, \Sigma, \widetilde{ p}$. Then the random variable defined by $ g = \mu \exp( x)$ obeys a distribution whose group-theoretic mean $ \mu_m$ and covariance $ \Sigma_m$ are estimated by
\begin{align*}
	m' &= \left\langle J_\ell ^{-1} \right\rangle ^{-1} m\\
	\mu_m &= \mu \exp( m'+O(|m'|^2)) \\
	\Sigma_m &= \Sigma - \sym\left( \left\langle J_{\ell ^{-1} m' x^{T}} \right\rangle \right) + O(|m'|^2) .
\end{align*}
\end{thm}

\begin{thm}
The group-theoretic mean $ \mu(t)$ and covariance $ \Sigma(t)$ of a stochastic process $ g(t)$ described by the McKean-Gangolli injection Ito's SDE obey the following equations:
 \begin{align*}
	\left( \mu^{-1} \dot{\mu} \right) ^\vee &\approx \left\langle J_{\ell}^{-1} \right\rangle^{-1} \left\langle \frac{1}{2} \frac{\partial J_r^{-1}}{\partial x_k} \left( HH^{T} J_r^{-T} e_k \right) + J_r^{-1} h^c \right\rangle  , \\
	\dot{ \Sigma} &\approx \left\langle \sym \left[ \left( \frac{1}{2} \frac{\partial J_r^{-1}}{\partial x_k} \left( HH^{T} J_r^{-T} \right) e_k - J_{\ell}^{-1}(\mu^{-1} \dot{\mu})^\vee + J_r^{-1}h^c \right)x^{T}  \right] + J_r^{-1}HH^{T}J_r^{-T} \right\rangle 
\end{align*}
Note the resemblance of this with its Euclidean counterpart.
\end{thm}

\begin{align*}
	J_l(q) &= \left[ \left( \frac{\partial g}{\partial q_1} g^{-1} \right) ^\vee , \cdots, \left( \frac{\partial g}{\partial q_n} g^{-1} \right)^\vee  \right],\\ 
	J_r(q) &= \left[ \left( g^{-1} \frac{\partial g}{\partial q_1}  \right) ^\vee , \cdots, \left( g^{-1} \frac{\partial g}{\partial q_n}  \right)^\vee  \right] 
\end{align*}
We shall use the Taylor second-order approximation of inverse Jacobians:
\begin{align*}
	J_{\ell}^{-1}(x) &= I - \frac{1}{2} [\ad_X] + \frac{1}{12} [\ad_X]^2 - \ldots\\
	J_r^{-1}(x) &= I + \frac{1}{2}[\ad_X] + \frac{1}{12}[\ad_X]^2 - \ldots
\end{align*}

We use the following tricks for the derivations:
\begin{enumerate}[label=(\arabic*)]
	\item Any linear term of $ x$ inside the expectation vanishes because we recenter the mean to 0 at all times.
\item For any symmetric matrix $ S$, we have $\ad_{ i} S e_i = 0 $.
\item The expansion of moments to approximate expectation (with hard-to-bound error):
\begin{align*}
	\left\langle M(x) \right\rangle = \left\langle A_0 + A_1^{i} x_i + A_2^{ij} x_i x_j + \ldots \right\rangle \approx A_0 + A_2^{ij} \left\langle x_i x_j \right\rangle .
\end{align*}
\item As long as $ (I-A)^{n} \to 0$ converges ($ A$ is close to  $ I$),  $ A^{-1} = \sum_{ k= 0}^{\infty} (I-A)^{k}$. For $ \left\langle J_l^{-1} \right\rangle^{-1}$, since the 1st order term vanish under expectation, setting $ k=1$ gives the 2nd order approximation.
\item Since $ \ad$ is linear in each argument, $ \ad_{ X} = \ad_{ (x_i E_i) } = x_i \ad_{ i}$.
\end{enumerate}
We compute and discard terms with order 3 or higher:
\begin{align*}
	\left(\mu^{-1} \dot{\mu} \right)^\vee &\approx \left\langle I - \underbrace{\frac{1}{2} [\ad_X]}_{(1)} + \frac{1}{12} [\ad_X]^2 \right\rangle^{-1} \bigg\langle \frac{1}{2} \left( \frac{1}{2} \ad_{ k} + \frac{1}{12} \left( \ad_{ k} [ \ad_{ X}] + [ \ad_{ X}] \ad_{ k} \right)\right) HH^{T} \ldots \\
					      &\left( I + \frac{1}{2}[\ad_X] + \frac{1}{12}[\ad_X]^2 \right)^{T} e_k + \left( I + \frac{1}{2}[\ad_X] + \frac{1}{12}[\ad_X]^2 \right) \left( h + \frac{\partial h^{c}}{\partial x_i} x_i + \frac{1}{2} \frac{\partial^2 h^{c}}{\partial { x_i} \partial x_j} x_i x_j   \right)    \bigg\rangle \\
					      &\approx \underbrace{ \left( I - \frac{1}{12} \left\langle [\ad_X]^2 \right\rangle  \right)}_{(4) }  \bigg\langle \underbrace{\frac{1}{4} \ad_{ k}HH^{T} e_k}_{(2)} + \underbrace{ \frac{1}{8} \ad_{ k} HH^{T} [ \ad_{ X}]^{T} e_k}_{ (1)} + \frac{1}{48} \ad_{ k} HH^{T} ([ \ad_{ X}]^2)^{T} e_k\\ 
					      &+ \underbrace{ \frac{1}{24} (\ad_{ k} [ \ad_{ X}] + [ \ad_{ X}] \ad_{ k})HH^{T} e_k}_{ (1)} + \underbrace{ \frac{1}{48} \ad_{ k} [ \ad_{ X}] HH^{T}[ \ad_{ X}]^{T} e_k}_{ (2)} + \frac{1}{48} [ \ad_{ X}] \ad_{ k} HH^{T} [ \ad_{ X}]^{T} e_k  \\
					      &+ h+ \underbrace{\frac{\partial h^{c}}{\partial x_i} x_i }_{(1)}+ \frac{1}{2} \frac{\partial^2 h^{c}}{\partial { x_i} \partial x_j} x_i x_j + \underbrace{ \frac{1}{2} [ \ad_{ X}] h}_{ (1)} + \frac{1}{2} x_j [ \ad_{ X}] \frac{\partial h^{c}}{\partial x_j} + \frac{1}{12} [ \ad_{ X}]^2 h  \bigg\rangle \\
					      &\approx h + \bigg\langle  \frac{1}{48} \ad_{ k} HH^{T} ([ \ad_{ X}]^2)^{T} e_k + \frac{1}{48} [ \ad_{ X}] \ad_{ k} HH^{T} [ \ad_{ X}]^{T} e_k +  \frac{1}{2} \frac{\partial^2 h^{c}}{\partial { x_i} \partial x_j} x_i x_j \\ 
					      &+ \frac{1}{2} x_j [ \ad_{ X}] \frac{\partial h^{c}}{\partial x_j} + \underbrace{ \frac{1}{12} [ \ad_{ X}]^2 h - \frac{1}{12} \langle [ \ad_{ X}]^2 \rangle h }_{ \text{cancel after next step} }  \bigg\rangle  \\
					      &\approx   h + \bigg(  \frac{1}{48} \ad_{ k} HH^{T}\ad_{ j}^{T} \ad_{ i}^{T} e_k + \frac{1}{48} \ad_{ i} \ad_{ k} HH^{T} \ad_{ j}^{T} e_k +  \frac{1}{2} \frac{\partial^2 h^{c}}{\partial { x_i} \partial x_j} \\ 
					      &+ \frac{1}{2} \ad_{ i} \frac{\partial h^{c}}{\partial x_j} \bigg) \langle x_i x_j\rangle \qquad \qquad \qquad \qquad    \text{ use (5) for all and then use (3)}   \\
					      &=: h + M^{\mu}_{ij} \Sigma_{ij}  .
 \\
\end{align*}

For the covariance, the same simplifications apply. Keeping only up to second-order terms, we have
\begin{align*}
	\dot{ \Sigma} & \approx \bigg\langle \sym \bigg\{ \bigg[ \frac{1}{2} \left( \frac{1}{2} \ad_{ k} + \frac{1}{12} ( \ad_{ k} [ \ad_{ X}] + \underbrace{ [ \ad_{ X}] \ad_{ k}}_{(2)}) \right) HH^{T} \left( I + \frac{1}{2} [ \ad_{ X}] \right)^{T} e_k  \\ 
		      &- \left( I- \frac{1}{2} [ \ad_{ X}] \right) \left( \mu^{-1} \dot{\mu} \right)^\vee + \left( I + \frac{1}{2} [ \ad_{ X}] \right) \left( h + \frac{\partial h^{c}}{\partial x_i} x_i \right)   \bigg] x^{T}  \bigg\} \\
		      &+  \left( I+ \frac{1}{2} [ \ad_{ X}] + \frac{1}{12} [ \ad_{ X}]^2 \right) HH^{T} \left( I+ \frac{1}{2} [ \ad_{ X}] + \frac{1}{12} [ \ad_{ X}]^2 \right) \bigg\rangle \\ 
		      &\approx HH^{T} + \bigg\langle \sym \bigg\{ \bigg[ \frac{1}{8} \ad_{ k} HH^{T} [ \ad_{ X}]^{T} e_k + \frac{1}{24} \ad_{ k} [ \ad_{ X}]HH^{T} e_k + \frac{1}{2} [ \ad_{ X}] \left( h + \left( \mu^{-1} \dot{\mu} \right)^\vee  \right) + \frac{\partial h^{c}}{\partial x_i} x_i \bigg] x^{T} \\ 
	&+ \frac{1}{12} [ \ad_{ X}]^2 HH^{T} \bigg\} + \frac{1}{4} [ \ad_{ X}] H H^{T} [\ad_{ X}]^{T} \bigg\rangle \\
		      &\approx  HH^{T} + \bigg( \sym \bigg\{ \bigg[ \frac{1}{8} \ad_{ k} HH^{T}  \ad_i^{T} e_k + \frac{1}{24} \ad_{ k} \ad_i HH^{T} e_k + \frac{1}{2} \ad_{ i} \left( h + \left( \mu^{-1} \dot{\mu} \right)^\vee  \right) + \frac{\partial h^{c}}{\partial x_i} \bigg] e_j^{T} \\ 
	&+ \frac{1}{12} \ad_{ i} \ad_{ j} HH^{T} \bigg\} + \frac{1}{4} \ad_{ i} H H^{T} \ad_{ j}^{T} \bigg) \left\langle x_i x_j \right\rangle \\
	&=: HH^{T} + M^{ \Sigma}_{ij} \Sigma_{ij} .
\end{align*}

\begin{align*}
	M^{\mu} &= \frac{1}{48} \ad_i \ad_k HH^{T} \ad_{ j} ^{T} e_k + \frac{1}{48} \ad_{ k} HH^{T} \ad_j ^{T} \ad_{ i}^{T} e_k + \frac{1}{2} \ad_{ i} \frac{\partial h^{c}}{\partial x_j} + \frac{1}{2} \frac{\partial^2 h^{c}}{\partial { x_i} \partial x_j}    \\
	M^{ \Sigma} &= \sym \bigg\{ \bigg[ \frac{1}{8} \ad_{ k} HH^{T}  \ad_i^{T} e_k + \frac{1}{24} \ad_{ k} \ad_i HH^{T} e_k + \frac{1}{2} \ad_{ i} \left( h + \left( \mu^{-1} \dot{\mu} \right)^\vee  \right) + \frac{\partial h^{c}}{\partial x_i} \bigg] e_j^{T} \\ 
	&+ \frac{1}{12} \ad_{ i} \ad_{ j} HH^{T} \bigg\} + \frac{1}{4} \ad_{ i} H H^{T} \ad_{ j}^{T} .
\end{align*}

We note that the only difference between equations from right FPE and from left FPE from the paper is that right FPE equations has all positive signs.


Let us now consider the Euler-Poincar\'{e} attitude dynamics with linear feedback control. The Lie group of interest is $ G = \SO(3) \ltimes \rr^{3}$, where the space of angular momenta $ \rr^{3}$, an additive Lie group, is identified with $ \mathfrak{so}(3)$ via the skew-symmetric operations $ ( \cdot )^\vee$ and $ ( \cdot )^\wedge $. Let $ R$ be the attitude in matrix form, $ \ell$ be the body-frame angular momentum, and  $ \omega$ be the body-frame angular velocity, so $ g = (R, \ell)$ with $ \ell = \mathbb{I} \omega$. Next, we present the exponential map $ \exp:  \mathfrak{g} \supset U \to V \subset G$ of $ G$. We also denote the exponential map of $ \SO(3)$ as $ \exp ( \xi) := \expm \left( \xi^\wedge \right) $. The input should resolve any ambiguity. We claim that TODO
\begin{align*}
	\exp \begin{pmatrix} \xi \\ x \end{pmatrix} = \begin{pmatrix} \exp( \xi )\\ J_l( \xi) x \end{pmatrix} ,
\end{align*}
where
\begin{align*}
	J_l(x) := I + \frac{1- \cos \norm{ x} }{ \norm{ x}^2 } x^\wedge + \frac{\norm{ x} - \sin \norm{ x}  }{ \norm{ x}^3 } ( x^\wedge )^2 
\end{align*}
is the left Jacobian of the $ \SO(3)$ exponential map. Note that $ J_l(0) = I$.

Assume control has the form $ u = K \ell$ and shares the same channel as the noise with scaling matrix $ B$. Then the stochastic control process is modeled by the following Stratonovich SDE (Ito's is non-physical):
\begin{align*}
	g^{-1}\d g &= h^s(g)\d t+ H \textcircled{s}  \d W,  
\end{align*}
where
\begin{align*}
	h^s \begin{pmatrix} R\\ \ell \end{pmatrix} = \begin{pmatrix} \mathbb{I} ^{-1} \ell \\ - \ell^\wedge  \times \mathbb{I}^{-1} \ell + BK \ell \end{pmatrix}  , \qquad H = \begin{pmatrix} 0 &0\\0& B \end{pmatrix} .
\end{align*}

To obtain the 2nd-order approximation propagation, we need to compute the Jacobian and Hessian of following function:
\begin{align*}
	h^c \begin{pmatrix} \xi \\ x \end{pmatrix} &:= h^s \left( g \exp \begin{pmatrix} \xi \\ x \end{pmatrix}  \right) \\
						   & = h^s \begin{pmatrix} \overline{R} \exp(\xi) \\ \overline{ \ell} + J_l(\xi)x \end{pmatrix}  \\
						   &=  \\
\end{align*}




\begin{align*}
	D( x^\wedge I^{-1} x) e_j = \left(-\left( I^{-1} x \right)^\wedge  + x^\wedge I^{-1} \right) e_j
\end{align*}
Then
\begin{align*}
	\partial x_i \left(-\left( I^{-1} x \right)^\wedge  + x^\wedge I^{-1} \right) e_j &= \partial x_i \left( (-I^{-1} x)^\wedge e_j \right) + E_i^{\SO(3)} I^{-1} e_j \\
	&= \partial x_i \left( e_j^\wedge I^{-1}x \right) + E_i^{\SO(3)} I^{-1} e_j\\
	&= E_j^{\SO(3)} I^{-1} e_i +  E_i^{\SO(3)} I^{-1} e_j .
\end{align*}
Since the term is symmetric, we can combine them into one when summed and scaled by entries of a symmetric matrix.

\section{Tangent Bundle of A Lie Group}
\subsection{Group Theory}
We first review two key concepts from group theory: group actions and semidirect products.
\begin{defn}
	Let $ G$ be a group and let  $ A$ be a set. A \allbold{group action} $ \rho: G \to \aut (A), g \mapsto \rho_g$ is a group homomorphism. That is, each element $ g \in G$ corresponds to an invertible ``permutation'' of $ A$, which means $ \rho_g(a)$ sends any element $ a$ to another element in $ A$ in an invertible way. There are two types of group actions, namely left and right actions. The former is denoted $ g.a = \rho_g(a)$ and satisfies $ (gh).a = g.(h.a)$, and the latter is denoted $ a .g = \rho_g(a)$ and satisfies $ a.(gh) = (a .g).h$.
\end{defn}
The idea of group action is that since group elements have inverses, compose associatively, and contain an identity element, we can use them to encode automorphisms of a set, which themselves forms a group and thus share the same behaviors. Thus, it is easy to check that $ (\rho_g)^{-1} = \rho_{g^{-1}}$. Since the action is only required to be a group homomorphism but not isomorphism, it is possible that multiple group elements encode the same automorphism.

If $ A = G$, then there are three natural actions of  $ G$ on itself: given any $ g$, we have left multiplication $ g.h = gh$, right multiplication $ h.g = hg$, and conjugation $ g.h = ghg^{-1}$.

In the context of a Lie group $ G$, left and right multiplications are smooth automorphisms of $ G$ that induces derivatives on the tangent spaces. If $ v_h \in T_hG $, then the induced derivative of left multiplication is $T_h L_g (v_h) $ and that of right multiplication is $ T_hR_g(v_h)$. Notice that we need to keep careful track of the base point. Together, they induce an action on the Lie algebra $ \mathfrak{g} := T_e G $ called the \allbold{adjoint action}. Given any $ \xi \in \mathfrak{g} $, the adjoint action of $ g$, $ \Ad_{ g}: \mathfrak{g} \to \mathfrak{g} $ is defined as 
\begin{align*}
	\Ad_{ g}( \xi) &:= T_{g^{-1}} L_{g} \circ  T_e R_{g^{-1}} (\xi), \\
	&:= T_g R_{g^{-1}} \circ  T_e L_g (\xi) .
\end{align*}
In a sense, the adjoint action is conjugation by the derivatives. Notice that the derivatives of left and right multiplications commute with each other, just as left and right multiplications themselves. We simply need to make sure the base point is correct.

We shall see later that the definition of $ \Ad$ can indeed be derived from differentiating group conjugation. For now, let us review semidirect products.
\begin{defn}
	Let $ (K,*_K), (H,*_H)$ be groups and suppose there exists a group action $ \rho: K \to \aut (H)$.  Then the \allbold{semidirect product} $K \ltimes_\rho H$ is the set $ K \times H$ with the identity element $ (e_K,e_H)$ and is endowed with the following the group operation $ *$ induced by the group action $ \rho$:
\begin{align*}
(k_1,h_1)*(k_2,h_2) = (k_1 *_K k_2, h_1 *_H \rho_{k_1}(h_2))
\end{align*}
Thus, $ (k,h)^{-1} = (k^{-1}, \rho_{k^{-1}}(h^{-1}))$ since $ (k,h)*(k^{-1},\rho_{k^{-1}}(h^{-1})) = (e_K, h *_H [\rho_k \circ \rho_{k^{-1}}(h^{-1})]) = (e_K, e_H)$, where we use the fact that $ \rho_{k^{-1}} = \rho_k^{-1}$.

Alternatively, we can define a different group operation $ *$ such that
\begin{align*}
	(k_1,h_1)*(k_2,h_2) = (k_1 *_K k_2, \rho_{k_2 ^{-1}}(h_1) *_H h_2).
\end{align*}
This is an equally valid definition yet is rarely mentioned in the literature. We will see that this definition is vital for our purpose so we haphazardly name it the \allbold{cosemidirect product} and denote it as $ K \ltimes_{\rho^{-1}}^* H$. Then, $ (k,h)^{-1} = (k^{-1}, \rho_{k}(h^{-1}))$ since
\begin{align*}
	(k,h)*(k^{-1}, \rho_{k} (h^{-1})) = (e_K, \rho_k(h) *_H \rho_k(h^{-1})) = (e_K, \rho_k (h *_H h^{-1})) = (e_K, e_H).
\end{align*}
\end{defn}
\begin{eg}
Consider the Lie groups $ (\SO(3), \text{matmul}) $ and $ (\rr^3,+)$. The 3D special Euclidean group is defined as $ \SE(3) := \SO(3) \ltimes _{\Ad} \rr^3$, where $ \Ad_{ R}( v) = Rv$. Then its group operation $ *$ is
\begin{align*}
	(R_1,v_1)*(R_2,v_2) = (R_1R_2, v_1 + \Ad_{ R_1}( v_2)) = (R_1R_2, v_1+ R_1 v_2). 
\end{align*}
Observe that we can embed $ \SE(3)$ into  $ \SL(4, \rr)$ and convert $ *$ to simply matrix multiplication:
 \begin{align*}
	 (R_1,v_1)*(R_2,v_2) \mapsto  \begin{pmatrix} R_1& v_1\\ 0 & 1 \end{pmatrix} \begin{pmatrix} R_2 & v_2\\ 0& 1 \end{pmatrix} = \begin{pmatrix} R_1R_2& v_1 + R_1v_2\\0&1 \end{pmatrix}  .
\end{align*}
\end{eg}
\begin{eg}
We define the 3D left special Euclidean group as $ \LSE(3) := \SO(3) \ltimes_{\Ad^{-1}}^* \rr^3$. Since $ R^{-1} = R^{T}$ in $ \SO(3)$, the group operation $ *$ is
 \begin{align*}
	(R_1,v_1)*(R_2,v_2) = (R_1R_2, \Ad_{ R_2^{T}}( v_1) + v_2) = (R_1R_2, R_2^{T} v_1 + v_2). 
\end{align*}
Since the group operation is different from that of $ \SE(3)$, its matrix embedding must be different as well. Consider the embedding $ (R,v) \mapsto \begin{pmatrix} R & 0\\v^{T}&1 \end{pmatrix} $, then $ *$ also becomes matrix multiplication:
\begin{align*}
	 (R_1,v_1)*(R_2,v_2) \mapsto  \begin{pmatrix} R_1& 0\\ v_1^{T} & 1 \end{pmatrix} \begin{pmatrix} R_2 & 0\\ v_2^{T}& 1 \end{pmatrix} = \begin{pmatrix} R_1R_2& 0\\ v_1^{T}R_2 + v_2^{T} &1 \end{pmatrix} \mapsto (R_1 R_2, R_2^{T}v_1 + v_2) .
\end{align*}
\end{eg}

\subsection{Lie Group Structure of Tangent Bundle}
The right trivialization results from this section can be found at \cite{MarsdenRatiu1999} Chapter 4.1 (note the right action semidirect product mentioned there is not the cosemidirect product we have here). TODO Engo 2001 mentions some results but did not cite nor elaborate. 

Let $ G$ be a Lie group of dimension $ n$. The goal of this section is to show that the tangent bundle $ TG$  has two group-structure-preserving trivializations (meaning we can parametrize the group by another group whose underlying manifold is the trivial vector bundle $ G\times \rr^{n}$), which we call \allbold{left and right trivializations}. First, notice that $ G$ acts on itself by left or right multiplication. The action induces an action on the fibers of $ TG$: $ g.(h,v) = (gh, T_h L_g(v))$ or $(h,v).g = (hg, T_hR_g(v))$.  This induced action allows us to move the base point of all tangent vectors of $ G$ to the identity, so that all tangent vectors in $ TG$ are translated to the Lie algebra $ \mathfrak{g}$. This way, we can define left and right trivialization maps as
\begin{align*}
	LT &: TG \to G \ltimes_{\Ad^{-1}}^*   \mathfrak{g}, (g,v) \mapsto (g, T_g L_{g^{-1}}(v)),\\
	RT &: TG \to G \ltimes_{\Ad}   \mathfrak{g}, (g,v) \mapsto (g, T_g R_{g^{-1}}(v)).
\end{align*}

We claim that the two maps are Lie group isomorphisms. That is, it is a diffeomorphism to the direct product manifold and respects the semidrect product structure. The inverse is $ (g,u)^{-1} = \left( g^{-1}, - \Ad_{ g}( u) \right)$. To prove the claim, we shall use the inverses of left and right trivialization maps instead, for convenience.
\begin{defn}
The left translation and right translation maps on $ TG$ are respectively defined as
\begin{align*}
	TL&: G\ltimes_{ \Ad^{-1}}^*  \mathfrak{ g} \to TG,\ (g,\xi) \mapsto (g,T_eL_g (\xi)), \\
	TR&: G \ltimes _{\Ad} \mathfrak{g} \to TG, (g, \xi) \mapsto (g, T_eR_g (\xi)) .
\end{align*}
\end{defn}

\begin{prop} 
Let $ G$ be a Lie group and  $ \mathfrak{ g }$ be its Lie algebra. Then the left and right translation maps $ TL$ and  $ TR$ are Lie group isomorphisms.
\end{prop}

\begin{proof}
Recall that multiplication on $ TG$ is the differential of the multiplication on $ G$. That is, if $ \mu: G \times G \to G, (g,h) \mapsto gh$, then product rule gives
\begin{align*}
	d\mu : TG \times TG \to TG,\ ((g, \xi) , (h, \eta)) \mapsto (gh, T_hL_g (\eta) + T_g R_h (\xi)).
\end{align*}

The left translation map $ TL: G\ltimes_{ \Ad^{-1}} \mathfrak{ g} \to TG,\ (g,\xi) \mapsto (g,T_eL_g \xi)$ is smooth since $d\mu$ is smooth, the map $(g,\xi) \mapsto  ((e,\xi),(g,0))$ is smooth, and $ d\mu ((e,\xi),(g,0)) = (g, T_eL_g(\xi) )$ is a composition of smooth maps. It is a diffeomorphism since it has inverse $LT: (g, v) \mapsto (g,T_g L_{g^{-1}}(v))$. It remains to show that it is also a Lie group homomorphism.

Since $G$ is a multiplicative group whereas $\mathfrak{g}$ as a vector space is an additive group, we have
\begin{align*}
    TL((g,\xi)*(h,\eta)) &= TL((gh,\ \Ad_{h^{-1}}(\xi) + \eta))\\
    &=(gh, T_e L_{gh}( \Ad_{ h^{-1}}( \xi) + \eta))\\
    &= (gh, T_e L_{gh} ( T_{h}L_{h^{-1}} \circ T_e R_{h} (\xi) + \eta))\\
    &= (gh, T_h L_g \circ T_eR_h(\xi) + T_hL_g \circ T_eL_h(\eta) )\\
    &= (gh, T_hL_g \circ T_eL_h(\eta) + T_gR_h \circ T_eL_g(\xi)) && \text{LR commute}  \\
    &= d\mu [(g, T_e L_g(\xi)) ,\ (h,T_eL_h (\eta)) ]\\
    &= d\mu [TL(g,\xi) ,\ TL(h,\eta)].
\end{align*}
Thus, $TL$ is a Lie group homomorphism and therefore an isomorphism. The results for $TR$ exactly mirror those of $ TL$. For completeness, we show
\begin{align*}
    TR((g,\xi)*(h,\eta)) &= TR((gh,\ \xi + \Ad_g(\eta)))\\
    &=(gh, T_e R_{gh}(\xi + \Ad_g(\eta)))\\
    &= (gh, T_e R_{gh} (\xi+ T_{g^{-1}}L_g \circ T_e R_{g^{-1}} (\eta)))\\
    &= (gh, T_gR_h \circ T_e R_g(\xi) + T_gR_h \circ T_eL_g(\eta))\\
    &= (gh, T_gR_h ( T_eR_g(\xi)) + T_hL_g (T_eR_h (\eta))) &&\text{LR commute} \\
    &= d\mu [(g, T_e R_g(\xi)) ,\ (h,T_eR_h (\eta)) ]\\
    &= d\mu [TR(g,\xi) ,\ TR(h,\eta)].
\end{align*}
\end{proof}

It turns out that both left and right trivializations yield the same Lie algebra! TODO: why? intuition? Thus we denote the Lie algebra by $ \mathfrak{g} \ltimes \mathfrak{g} $, which we claim is endowed with the following Lie bracket:
\begin{align*}
	[(\xi, u), (\eta,v)] := \left( [\xi, \eta], [\xi,v ] - [\eta, u] \right) .
\end{align*}
Its \allbold{adjoint representation} $ \ad_{ (\xi,u)} := [(\xi,u), -] : \mathfrak{g} \ltimes \mathfrak{g} \to \mathfrak{g} \ltimes \mathfrak{g} $. 
Note that the notations $ \Ad, \ad, [ -, - ] $ can come from $ G$ or the semidirect product, and any ambiguity can be resolved by inspecting their inputs. Now we derive the bracket formula.

Loosely speaking, the adjoint action $ \Ad$ is the derivative of group conjugation action at identity, and adjoint representation $ \ad$ is the derivative of adjoint action at identity. We make this precise in the context of the tangent bundle of $ G$: for any $ (g,u) \in G \ltimes_{\Ad^{-1}}^* \mathfrak{g} $, its action on $ G \ltimes_{\Ad^{-1}}^*  \mathfrak{g} $ by conjugation is
\begin{align*}
	\phi_{(g,u)} : G \ltimes_{\Ad^{-1}}^* \mathfrak{g}  \to G \ltimes_{\Ad^{-1}}^* \mathfrak{g} , (h,v) \mapsto (g,u)*(h,v)*(g,u)^{-1}.
\end{align*}
Explicitly, we have
\begin{align*}
	\phi_{(g,u)} (h,v) &= (g,u)*(h,v)*\left( g^{-1}, - \Ad_{ g}( u) \right)  \\
	&= (gh, \Ad_{ h^{-1}}( u) + v) * \left( g^{-1}, - \Ad_{ g}( u) \right)  \\
	&= \left( ghg^{-1}, \Ad_{ g}( \Ad_{ h^{-1}}( u) + v) - \Ad_{ g}( u) \right)  .
\end{align*}
Let $ (\gamma, c)$ be a smooth curve on $ G \times  \mathfrak{g} $ such that $ \gamma(0) = e,\ \dot{ \gamma}(0) = \eta \in \mathfrak{g},\ c(0) = 0,\ \dot{c}(0) = v$. Then the derivative of $ \phi_{(g,u)}$ at $ (e,0)$, the adjoint action $ \Ad_{(g,u)} : \mathfrak{g} \ltimes  \mathfrak{g}  \to \mathfrak{g}\ltimes  \mathfrak{g} $ is given by
 \begin{align*}
	 \Ad_{ (g,u)}(\eta, v ) &= \frac{d}{dt}\big|_{t= 0} \phi_g ( \gamma(t), c(t)) \\
				&= \frac{d}{dt}\big|_{t= 0} \left( g \gamma(t) g^{-1}, \Ad_{ g}( \Ad_{ \gamma(t)^{-1}}( u) + c(t)) - \Ad_{ g}( u) \right)  \\ 
						     &= \left( \Ad_{ g}( \eta), \Ad_{ g}(- \ad_{\dot{ \gamma}(0)} (u) + \dot{c}(0) ) \right)  \\
						     &= \left( \Ad_{ g}( \eta), \Ad_{ g}( -[ \eta, u] + v) \right)  ,
\end{align*}
where we use the fact that $ \ad_{ \dot{ \gamma}(0)} := \frac{d}{dt}\big|_{t= 0} \Ad_{ \gamma(t)}$ and the following result: 
\begin{align*}
\Ad_{ \gamma(t)^{-1}} \circ  \Ad_{ \gamma(t)} &= \id  \\
\frac{d}{dt}\big|_{t= 0} \left( \Ad_{ \gamma(t)^{-1}} \circ  \Ad_{ \gamma(t)} \right) &= 0 \\
\frac{d}{dt}\big|_{t= 0} \Ad_{ \gamma(t)^{-1}} \circ  \Ad_{ \gamma(0)} + \Ad_{ \gamma(0)^{-1}} \circ \ad_{ \dot{ \gamma}(0)} &= 0 && \text{ chain rule}\\
\frac{d}{dt}\big|_{t= 0} \Ad_{ \gamma(t)^{-1}} \circ \Ad_{ e} + \Ad_{ e} \circ \ad_{\dot{ \gamma}(0)} &= 0 \\
\frac{d}{dt}\big|_{t= 0} \Ad_{ \gamma(t)^{-1}} &= - \ad_{ \dot{ \gamma}(0)}  && \Ad_{ e} = \id  . 
\end{align*}
Finally, let $ (\gamma, c)$ be another smooth curve on $ G \times  \mathfrak{g}$ such that $ \gamma(0) = e,\ \dot{ \gamma}(0) = \xi,\ c(0) =0,\ \dot{c}(0) = u$. Then the derivative of $ \Ad_{ ( \gamma(t), c(t))}$ at $ t=0$ is the adjoint representation $ \ad_{ (\xi, u)}: \mathfrak{g} \ltimes^*  \mathfrak{g} \to \mathfrak{g} \ltimes^*  \mathfrak{g} $:
\begin{align*}
	\ad_{ (\xi,u)}(\eta, v) &= \frac{d}{dt}\big|_{t= 0} \Ad_{ ( \gamma(t), c(t))}( \eta,v ) \\
				&= \frac{d}{dt}\big|_{t= 0} \left( \Ad_{ \gamma(t)}( \eta), \Ad_{ \gamma(t)}( -[\eta, c(t)] + v) \right)    \\
				&= \left( \ad_{ \xi}(\eta), \ad_{ \xi} (-[\eta,c(0)]) + \Ad_{ \gamma(0)}( - [\eta, \dot{c}(0)] ) + \ad_{ \xi}(v) \right)   \\
				&= ([ \xi, \eta], [\xi, v] - [\eta, u]) .
\end{align*}

We collect the above results into a proposition:
\begin{prop}
The left trivialization $ G \ltimes_{\Ad^{-1}}^* \mathfrak{g} $ of $ TG$ has
 \begin{align*}
	\Ad_{ (g,u)}(\eta, v ) &= \left( \Ad_{ g}( \eta), \Ad_{ g}(v-[ \eta, u]) \right),\\
	\ad_{ (\xi,u)}(\eta, v) &=  ([ \xi, \eta], [\xi, v] - [\eta, u]). 
\end{align*}
\end{prop}
\begin{remark}
The right trivialization $ G \ltimes_{\Ad} \mathfrak{g} $ instead has
\begin{align*}
	\Ad_{ (g,u)}(\eta, v ) &= \left( \Ad_{ g}( \eta), \Ad_{ g}(v)-[ \Ad_{ g}( \eta), u]) \right).
\end{align*}
This is different from that of the left trivialization. However, the adjoint representation is exactly the same. Moreover, $ \SE(3)$ manifests this structure for $ \TSO(3)$ through matrix multiplication. However, body-frame dynamics demand left trivialization, so we are stuck with the cosemidirect product $ \LSE(3)$.
\end{remark}

Recall that any finite-dimensional Lie algebra is a finite-dimensional vector space and thus is isomorphic to $ \rr^{n}$ after choosing a basis, and we convert back and forth using $ x^\wedge = X \in \mathfrak{g} $ and $ X^\vee = x \in \rr^{n}$. 

The Lie algebra elements of $ \mathfrak{lse}(3) = \mathfrak{so}(3) \ltimes \mathfrak{so}(3) \cong \rr^3 \ltimes \rr^3$ have the following matrix embedding:
\begin{align*}
	(\xi, v) \mapsto \begin{pmatrix} \xi^\wedge & 0\\ v^{T}&0 \end{pmatrix} ,
\end{align*}
where $ \xi^\wedge $ is a skew-symmetric matrix in $ \mathfrak{so}(3) $.

Now we have a matrix representation, we can easily verify the $ \Ad$ and  $ \ad$ maps for $ \LSE(3)$ using matrix multiplication.
\begin{align*}
	\Ad_{ (R,u)}( \eta,v)  &= (R,u) * (\eta,v) * (R,u)^{-1}   \\
			       &\mapsto \begin{pmatrix} R&0\\ u^{T} &1 \end{pmatrix}\begin{pmatrix} \eta^\wedge & 0 \\ v^{T}&0 \end{pmatrix}   \begin{pmatrix} R^{T}& 0\\ - Ru^{T}R^{T} & 1 \end{pmatrix}  \\
			       &= \begin{pmatrix} R \eta^\wedge & 0\\ u^{T} \eta^\wedge + v^{T} & 0 \end{pmatrix} \begin{pmatrix} R^{T}&0\\- Ru^{T}R^{T}&1 \end{pmatrix}  \\
			       &= \begin{pmatrix} R \eta^\wedge R^{T} & 0 \\ (u^{T} \eta^\wedge + v^{T})R^{T}&0\end{pmatrix}  \\
			       &\mapsto  (R \eta^\wedge R^{-1}, R(v - \eta^\wedge u)) && (\eta^\wedge )^{T} = - \eta^\wedge \\
			       &\mapsto  ( \Ad_{ R}( \eta  ), \Ad_{ R}( v - [\eta,u])) .
\end{align*}
In the last step, we use the following facts: 1. adjoint action of matrix Lie group is literally conjugation, $ \Ad_{ R}( \Omega) = R\Omega R^{-1}$;  2. adjoint action of matrix Lie group on $ \mathfrak{g} $ identified as $ \rr^{n}$ is simply matrix-vector multiplication, $ \Ad_{ R}( \omega) = R \omega = R \omega^\wedge R^{-1} $.

The adjoint representation of matrix Lie group is just the commutator $ \ad_{ X}(Y) := [X,Y] = XY - YX$: 
\begin{align*}
	\ad_{ ( \xi, u)} (\eta,v) &\mapsto  \begin{pmatrix} \xi^\wedge & 0\\u^{T}&0 \end{pmatrix}  \begin{pmatrix}  \eta^\wedge &0\\v^{T}&0 \end{pmatrix} - \begin{pmatrix}  \eta^\wedge &0\\v^{T}&0 \end{pmatrix} \begin{pmatrix} \xi^\wedge & 0\\u^{T}&0 \end{pmatrix}\\
				  &= \begin{pmatrix} \xi^\wedge \eta^\wedge & 0\\ u^{T} \eta^\wedge &0 \end{pmatrix} -  \begin{pmatrix} \eta^\wedge \xi^\wedge & 0\\ v^{T} \xi^\wedge &0 \end{pmatrix}  \\
				  &= \begin{pmatrix} \xi^\wedge \eta^\wedge - \eta^\wedge \xi^\wedge &0 \\ \left( (\eta^\wedge)^{T}u - (\xi^\wedge )^{T} v  \right)^{T} &0 \end{pmatrix}  \\
				  &\mapsto  \left( [ \xi, \eta], \xi^\wedge v - \eta^\wedge u  \right) && \xi, \eta \text{ skew-symmetric}  \\
				  &= \left( [ \xi, \eta], [\xi,v] - [\eta,u] \right)  .
\end{align*}
The results match those from the general case!

Using this matrix representation of $ \LSE(3)$, we can apply the propagation formula.

A natural basis for $ \mathfrak{lse}(3) $ is
 \begin{align*}
	E_i = \begin{cases}
		\begin{pmatrix} E_i^{ \mathfrak{so}(3)}&0\\0&0 \end{pmatrix} & i = 1,2,3,\\
		\begin{pmatrix} 0& 0 \\ e_{i-3}^{T} & 0 \end{pmatrix} & i = 4,5,6 . 
	\end{cases}
\end{align*}
Recall that $ \ad_{ i} := [\ad_{ E_i}]$ is the matrix that represents the linear operator $ \ad_{ E_i}$, whose $ j$th column can be computed as $ [E_i, E_j]^\vee$.

\section{Intuition of Hamiltonian Mechanics}
A symplectic manifold has a conanical 2-form $ \omega$, which locally (in Darboux coordinates) acts as a symplectic bilinear form $J:= \begin{pmatrix} 0&I\\-I & 0 \end{pmatrix} $. Intuitively this ``turns the tangent coordinates 90 degrees". That is, horizontal direction becomes vertical and vice versa (not too different from $ \begin{pmatrix} 0&1\\-1&0 \end{pmatrix} $ if we think $ x$-coordinate as horizontal tangent coordinates and  $ y$-coordinate as vertical tangent coordinates). When this is applied to gradient vector field  of a function $ H$, since gradient goes perpendicular to the contour lines of $ H$, by turning 90 degrees now the vector field goes along the contour lines (level sets), which means that along the flow of the rotated vector field, $ H$ is conserved. This vector field is the Hamiltonian vector field $ X_H$ of  $ H$. Bloch Exercise 3.1.2 shows that in local Darboux coordinates, indeed we have $ X_H = J df^{T} = J \nabla f$. However, since there is no metric on the manifold, we cannot actually obtain gradient vector field of a function nor rotate it. Yet we can still do this rotation to flow along the contour lines thanks to the 2-form. Instead of rotating the gradient vector field, we rotate the differential contravariantly and get a vector field via $ \iota_{{X_H}} \omega$.

Moreover, when the concept is applied to the cotangent bundle of a manifold, we have a canonical 1-form $ \theta$ and a canonical 2-form $ \omega := -d \theta$ that is symplectic. We can write this 2-form in local coordinates, and then compute the $ X_H$ in local coordinates as well for any $ H$. If we choose $ H$ to be the energy function, then the resulting coordinates are exactly Hamilton's equations, which give us the flows where energy is conserved. Notice that we do not need to use calculus of variations at all here. Geometry completely determines the equations of motion.

\begin{align*}
	\dot{z} &= X_H(z) \\
	\dot{f}(z) &= \left\langle df(z), X_H(z) \right\rangle \ \forall \ f\\
	\dot{f} &= \left\{ f,H \right\} \ \forall \ f
\end{align*}
\end{document}
