\documentclass[12pt,class=article,crop=false]{standalone} 
\input{../preamble.tex}

\begin{document}
\subsection{Invariant EKF}

\begin{defn}
Let $ G$ be a Lie group and $ f_u: G \to TG$ be a input-dependent vector field. Then $ f_u$ is  \allbold{group-affine} if it satisfies 
\begin{align*}
	f_u(ab) = f_u(a) b + a f_u(b) - a f_u( Id) b  \in T_{ab}G.
\end{align*}
\end{defn}
Group-affine is the weakest assumption one needs on the dynamics to have trajectory-independent error with log-linear error dynamics.

Given two state trajectories $ X,\wh{ X}$ that follows the same deterministic dynamics, let $ \eta^{L} := X^{-1}\wh{ X}$ and $ \eta^R := \wh{ X} X^{-1} $ be their left and right errors. Then the first theorem says that $ f_u$ is group affine iff $ \eta^L$ is state trajectory-independent iff $ \eta^R$ is state trajectory-independent.

\begin{thm}
Let $ i=L,R$. Let  $ \xi^i_0 = (\log(\eta_0^{i}))^\vee$. Define $ A^{i}$ as
\begin{align*}
	g_u( \exp( \xi)) = \left( A^{i} \xi \right)^\vee + O(\norm{ \xi}^2). 
\end{align*}
If $ \xi^{i}$ is defined $ \ \forall \ t>0$ by the linear diffeq in $ \rr^{n}$ :
\begin{align*}
	\frac{d}{dt} \xi^{i} = A^{i} \xi^{i} .
\end{align*}
Then for the true nonlinear error $ \eta^{i}$, we have $ \eta^{i} = \exp( \xi^{i}) \ \forall \ t \geq 0$.
\end{thm}

Let $ \eta = X^{-1} \wh{ X} $ be the left-invariant error. In EKF, $ \wh{ X}$ represents estimated state and $ X$ is the true state, where they share the same deterministic dynamics and only differ in initial state. In the context of uncertainty propagation, $ X$ is modeled by a stochastic process governed by an SDE, and $ \wh{ X}$ is the propagated mean.

When the dynamics $ f_u$ is group-affine, the evolution of the error $ \eta$ satisfies
\begin{align*}
	\frac{d}{dt} \eta = g_u (\eta)
\end{align*}
where $ g_u$ satisfies $ g_u(\eta) = f_u(\eta) - f_u( Id) \eta$. It turns out that $ g_u$ can be converted to a linear dynamic under the exponential coordinates. Let  $ (A \xi) ^\wedge$ be the first-order approximation of  $ g_u (\exp (\xi))$. Then we have $ \eta = \exp( \xi)$ for all time where
\begin{align*}
	\frac{d}{dt} \xi = A \xi .
\end{align*}
This means that the error $ \eta$ is trajectory-independent and can be linearized in the Lie algebra without approximation. 

Note that if $ f_u$ is affine (\emph{i.e.} $ f_u(x) = f_0 + Ax + Bu$), we know that the error has the linear dynamics  $ \dot{e} = A e$, independent of the state-trajectory. This is a generalization of that.

So in a stochastic setting, if the drift of SDE is group-affine, then no matter how state deviates from the mean, the drift error is trajectory-independent and can be propagated using the same ODE in the Lie algebra. If the drift is not group-affine, then using the drift equation (Ricatti?) along the mean trajectory should give additional error.

Euler-Poincare Equation:
\begin{align*}
	\mathbb{I} \dot{\xi} = \ad_{ \xi}^* ( \mathbb{I} \xi) + u 
\end{align*}
This can be linearized and then used for trajectory tracking. This requires already having a nominal trajectory, but in covariance steering we have to produce the nominal.

\subsection{Uncertainty Propagation on Lie Groups}

In the Euclidean case, an SDE $ dx = h(x,t) dt + H(t) dW$ has the following evolution of the first two moments:
 \begin{align*}
	\begin{cases}
		 \dot{\mu} = \left\langle h \right\rangle ,\\
		 \dot{\Sigma} = \left\langle h (x-\mu)^{T} + (x-\mu) h^{T} \right\rangle + HH^{T} .
	\end{cases}
\end{align*}
We can define SDE on $ G$ in three different ways. The first is the McKean-Gangolli injection:
 \begin{align*}
	g(t+ \d t) = g(t) \exp( h(g(t),t) \d t + H(g(t+k\d t), t+k\d t) \d W) .
\end{align*}
If $ k=0$, then the SDE is Ito's. If  $ k=\frac{1}{2}$, then it is Stratonovich's, which we distinguish by using $ \textcircled{s}$.

Stratonovich is natural and obeys the geometry and chain rule. However, it is extremely difficult to evaluate the expectation TODO. Thus, we rely on Ito's to compute expectations and then convert to Stratonovich's if needed. The conversion is
\begin{align*}
	h = h^s + \frac{1}{2} E_i^r\left( H_{kj}^s \right) H_{ij}^s e_k , \qquad H = H^s .
\end{align*}

We shall use the simplified notation
\begin{align*}
	(g^{-1} \d g)^\vee &= h\d t + H\d W\\
	(g^{-1} \d g)^\vee &= h^s \d t + H^s \textcircled{s} \d W .
\end{align*}


The second way is to parametrize group element $ g = g(q)$ and write
 \begin{align*}
	q(t+\d t) = q(t) + (J_r^{-1} \widetilde{ h})(q,t) \d t + (J_r^{-1} \widetilde{ H})(q(t+k\d t),t + k\d t) \d W .
\end{align*}

Using the exponential coordinates $ g(x) = \mu(t) \exp( x)$, under Stratonovich interpretation, to describe the same stochastic process, the drift and diffusion of both ways equal each other. Under Ito's interpretation, there is an additional drift term coming from nonlinearity of exponential map.

\begin{align*}
	\widetilde{ h}(x,t) = h(x,t) + \left( \frac{1}{2} J_r \frac{\partial J_r^{-1}}{\partial x_k} H H^{T} J_r^{-T}  \right) \bigg|_{g=\mu \exp( (x))} e_k,\\
	\widetilde{ H}(x,t) = H(\mu \exp( x),t) .
\end{align*}
Note that if $ H$ does not depend on the state, then Ito's and Stranotovich coincide for $ H$. However,  $ J_r$ could still be different under the two interpretations.

In reality, we are given $ h^s$ and  $ H^s$ TODO, which describe an SDE in the tangent space at each  $ g$. We would have to compute the $ h, H$ or  $ \widetilde{ h}, \widetilde{ H}$ that would yield the same stochastic process.


If $ g$ is not near the identity, then the exponential coordinates might not be convenient to use because of high nonlinearity of exponential map away from the identity and non-bijectiveness of the exponential map. In this case, a moving coordinate frame by re-centering solves the issue. However, a moving frame makes it harder to do covariance steering. We might have to do covariance steering on the error space.

The goal is to derive the error mean and covariance dynamics using the exponential coordinates. We can achieve this via computing the Fokker-Planck equation of 

\begin{defn}
Let $ p$ be a probability density function on an unimodular Lie group $ G$. The  \allbold{group-theoretic mean} $ \mu$ is defined by
\begin{align*}
	\int_G \log ^\vee \left( \mu^{-1} g \right) p(g) \d g = 0 .
\end{align*}
The covariance of $ p$ is defined by
 \begin{align*}
	\Sigma = \int_G \left[ \log ^\vee (\mu^{-1} g) \right] \left[ \log ^\vee (\mu^{-1}g) \right] ^{T} p(g) \d g .
\end{align*}
\end{defn}

\begin{thm}
Let $ x \in \rr^{N}$ be a random variable with concentrated probability distribution around the origin of $ D$. Let its mean, covariance, and probability density function be  $ m, \Sigma, \widetilde{ p}$. Then the random variable defined by $ g = \mu \exp( x)$ obeys a distribution whose group-theoretic mean $ \mu_m$ and covariance $ \Sigma_m$ are estimated by
\begin{align*}
	m' &= \left\langle J_\ell ^{-1} \right\rangle ^{-1} m\\
	\mu_m &= \mu \exp( m'+O(|m'|^2)) \\
	\Sigma_m &= \Sigma - \sym\left( \left\langle J_{\ell ^{-1} m' x^{T}} \right\rangle \right) + O(|m'|^2) .
\end{align*}
\end{thm}

\begin{thm}
The group-theoretic mean $ \mu(t)$ and covariance $ \Sigma(t)$ of a stochastic process $ g(t)$ described by the McKean-Gangolli injection Ito's SDE obey the following equations:
 \begin{align*}
	\left( \mu^{-1} \dot{\mu} \right) ^\vee &\approx \left\langle J_{\ell}^{-1} \right\rangle^{-1} \left\langle \frac{1}{2} \frac{\partial J_r^{-1}}{\partial x_k} \left( HH^{T} J_r^{-T} e_k \right) + J_r^{-1} h^c \right\rangle  , \\
	\dot{ \Sigma} &\approx \left\langle \sym \left[ \left( \frac{1}{2} \frac{\partial J_r^{-1}}{\partial x_k} \left( HH^{T} J_r^{-T} \right) e_k - J_{\ell}^{-1}(\mu^{-1} \dot{\mu})^\vee + J_r^{-1}h^c \right)x^{T}  \right] + J_r^{-1}HH^{T}J_r^{-T} \right\rangle 
\end{align*}
Note the resemblance of this with its Euclidean counterpart.
\end{thm}

\begin{align*}
	J_l(q) &= \left[ \left( \frac{\partial g}{\partial q_1} g^{-1} \right) ^\vee , \cdots, \left( \frac{\partial g}{\partial q_n} g^{-1} \right)^\vee  \right],\\ 
	J_r(q) &= \left[ \left( g^{-1} \frac{\partial g}{\partial q_1}  \right) ^\vee , \cdots, \left( g^{-1} \frac{\partial g}{\partial q_n}  \right)^\vee  \right] 
\end{align*}
We shall use the Taylor second-order approximation of inverse Jacobians:
\begin{align*}
	J_{\ell}^{-1}(x) &= I - \frac{1}{2} [\ad_X] + \frac{1}{12} [\ad_X]^2 - \ldots\\
	J_r^{-1}(x) &= I + \frac{1}{2}[\ad_X] + \frac{1}{12}[\ad_X]^2 - \ldots
\end{align*}

We use the following tricks for the derivations:
\begin{enumerate}[label=(\arabic*)]
	\item Any linear term of $ x$ inside the expectation vanishes because we recenter the mean to 0 at all times.
\item For any symmetric matrix $ S$, we have $\ad_{ i} S e_i = 0 $.
\item The expansion of moments to approximate expectation (with hard-to-bound error):
\begin{align*}
	\left\langle M(x) \right\rangle = \left\langle A_0 + A_1^{i} x_i + A_2^{ij} x_i x_j + \ldots \right\rangle \approx A_0 + A_2^{ij} \left\langle x_i x_j \right\rangle .
\end{align*}
\item As long as $ (I-A)^{n} \to 0$ converges ($ A$ is close to  $ I$),  $ A^{-1} = \sum_{ k= 0}^{\infty} (I-A)^{k}$. For $ \left\langle J_l^{-1} \right\rangle^{-1}$, since the 1st order term vanish under expectation, setting $ k=1$ gives the 2nd order approximation.
\item Since $ \ad$ is linear in each argument, $ \ad_{ X} = \ad_{ (x_i E_i) } = x_i \ad_{ i}$.
\end{enumerate}
We compute and discard terms with order 3 or higher:
\begin{align*}
	\left(\mu^{-1} \dot{\mu} \right)^\vee &\approx \left\langle I - \underbrace{\frac{1}{2} [\ad_X]}_{(1)} + \frac{1}{12} [\ad_X]^2 \right\rangle^{-1} \bigg\langle \frac{1}{2} \left( \frac{1}{2} \ad_{ k} + \frac{1}{12} \left( \ad_{ k} [ \ad_{ X}] + [ \ad_{ X}] \ad_{ k} \right)\right) HH^{T} \ldots \\
					      &\left( I + \frac{1}{2}[\ad_X] + \frac{1}{12}[\ad_X]^2 \right)^{T} e_k + \left( I + \frac{1}{2}[\ad_X] + \frac{1}{12}[\ad_X]^2 \right) \left( h + \frac{\partial h^{c}}{\partial x_i} x_i + \frac{1}{2} \frac{\partial^2 h^{c}}{\partial { x_i} \partial x_j} x_i x_j   \right)    \bigg\rangle \\
					      &\approx \underbrace{ \left( I - \frac{1}{12} \left\langle [\ad_X]^2 \right\rangle  \right)}_{(4) }  \bigg\langle \underbrace{\frac{1}{4} \ad_{ k}HH^{T} e_k}_{(2)} + \underbrace{ \frac{1}{8} \ad_{ k} HH^{T} [ \ad_{ X}]^{T} e_k}_{ (1)} + \frac{1}{48} \ad_{ k} HH^{T} ([ \ad_{ X}]^2)^{T} e_k\\ 
					      &+ \underbrace{ \frac{1}{24} (\ad_{ k} [ \ad_{ X}] + [ \ad_{ X}] \ad_{ k})HH^{T} e_k}_{ (1)} + \underbrace{ \frac{1}{48} \ad_{ k} [ \ad_{ X}] HH^{T}[ \ad_{ X}]^{T} e_k}_{ (2)} + \frac{1}{48} [ \ad_{ X}] \ad_{ k} HH^{T} [ \ad_{ X}]^{T} e_k  \\
					      &+ h+ \underbrace{\frac{\partial h^{c}}{\partial x_i} x_i }_{(1)}+ \frac{1}{2} \frac{\partial^2 h^{c}}{\partial { x_i} \partial x_j} x_i x_j + \underbrace{ \frac{1}{2} [ \ad_{ X}] h}_{ (1)} + \frac{1}{2} x_j [ \ad_{ X}] \frac{\partial h^{c}}{\partial x_j} + \frac{1}{12} [ \ad_{ X}]^2 h  \bigg\rangle \\
					      &\approx h + \bigg\langle  \frac{1}{48} \ad_{ k} HH^{T} ([ \ad_{ X}]^2)^{T} e_k + \frac{1}{48} [ \ad_{ X}] \ad_{ k} HH^{T} [ \ad_{ X}]^{T} e_k +  \frac{1}{2} \frac{\partial^2 h^{c}}{\partial { x_i} \partial x_j} x_i x_j \\ 
					      &+ \frac{1}{2} x_j [ \ad_{ X}] \frac{\partial h^{c}}{\partial x_j} + \underbrace{ \frac{1}{12} [ \ad_{ X}]^2 h - \frac{1}{12} \langle [ \ad_{ X}]^2 \rangle h }_{ \text{cancel after next step} }  \bigg\rangle  \\
					      &\approx   h + \bigg(  \frac{1}{48} \ad_{ k} HH^{T}\ad_{ j}^{T} \ad_{ i}^{T} e_k + \frac{1}{48} \ad_{ i} \ad_{ k} HH^{T} \ad_{ j}^{T} e_k +  \frac{1}{2} \frac{\partial^2 h^{c}}{\partial { x_i} \partial x_j} \\ 
					      &+ \frac{1}{2} \ad_{ i} \frac{\partial h^{c}}{\partial x_j} \bigg) \langle x_i x_j\rangle \qquad \qquad \qquad \qquad    \text{ use (5) for all and then use (3)}   \\
					      &=: h + M^{\mu}_{ij} \Sigma_{ij}  .
 \\
\end{align*}

For the covariance, the same simplifications apply. Keeping only up to second-order terms, we have
\begin{align*}
	\dot{ \Sigma} & \approx \bigg\langle \sym \bigg\{ \bigg[ \frac{1}{2} \left( \frac{1}{2} \ad_{ k} + \frac{1}{12} ( \ad_{ k} [ \ad_{ X}] + \underbrace{ [ \ad_{ X}] \ad_{ k}}_{(2)}) \right) HH^{T} \left( I + \frac{1}{2} [ \ad_{ X}] \right)^{T} e_k  \\ 
		      &- \left( I- \frac{1}{2} [ \ad_{ X}] \right) \left( \mu^{-1} \dot{\mu} \right)^\vee + \left( I + \frac{1}{2} [ \ad_{ X}] \right) \left( h + \frac{\partial h^{c}}{\partial x_i} x_i \right)   \bigg] x^{T}  \bigg\} \\
		      &+  \left( I+ \frac{1}{2} [ \ad_{ X}] + \frac{1}{12} [ \ad_{ X}]^2 \right) HH^{T} \left( I+ \frac{1}{2} [ \ad_{ X}] + \frac{1}{12} [ \ad_{ X}]^2 \right) \bigg\rangle \\ 
		      &\approx HH^{T} + \bigg\langle \sym \bigg\{ \bigg[ \frac{1}{8} \ad_{ k} HH^{T} [ \ad_{ X}]^{T} e_k + \frac{1}{24} \ad_{ k} [ \ad_{ X}]HH^{T} e_k + \frac{1}{2} [ \ad_{ X}] \left( h + \left( \mu^{-1} \dot{\mu} \right)^\vee  \right) + \frac{\partial h^{c}}{\partial x_i} x_i \bigg] x^{T} \\ 
	&+ \frac{1}{12} [ \ad_{ X}]^2 HH^{T} \bigg\} + \frac{1}{4} [ \ad_{ X}] H H^{T} [\ad_{ X}]^{T} \bigg\rangle \\
		      &\approx  HH^{T} + \bigg( \sym \bigg\{ \bigg[ \frac{1}{8} \ad_{ k} HH^{T}  \ad_i^{T} e_k + \frac{1}{24} \ad_{ k} \ad_i HH^{T} e_k + \frac{1}{2} \ad_{ i} \left( h + \left( \mu^{-1} \dot{\mu} \right)^\vee  \right) + \frac{\partial h^{c}}{\partial x_i} \bigg] e_j^{T} \\ 
	&+ \frac{1}{12} \ad_{ i} \ad_{ j} HH^{T} \bigg\} + \frac{1}{4} \ad_{ i} H H^{T} \ad_{ j}^{T} \bigg) \left\langle x_i x_j \right\rangle \\
	&=: HH^{T} + M^{ \Sigma}_{ij} \Sigma_{ij} .
\end{align*}

\begin{align*}
	M_{ij}^{\mu} &= \frac{1}{48} \ad_i \ad_k HH^{T} \ad_{ j} ^{T} e_k + \frac{1}{48} \ad_{ k} HH^{T} \ad_j ^{T} \ad_{ i}^{T} e_k + \frac{1}{2} \ad_{ i} \frac{\partial h^{c}}{\partial x_j} + \frac{1}{2} \frac{\partial^2 h^{c}}{\partial { x_i} \partial x_j}    \\
	M_{ij}^{ \Sigma} &= \sym \bigg\{ \bigg[ \frac{1}{8} \ad_{ k} HH^{T}  \ad_i^{T} e_k + \frac{1}{24} \ad_{ k} \ad_i HH^{T} e_k + \frac{1}{2} \ad_{ i} \left( h + \left( \mu^{-1} \dot{\mu} \right)^\vee  \right) + \frac{\partial h^{c}}{\partial x_i} \bigg] e_j^{T} \\ 
	&+ \frac{1}{12} \ad_{ i} \ad_{ j} HH^{T} \bigg\} + \frac{1}{4} \ad_{ i} H H^{T} \ad_{ j}^{T} .
\end{align*}

We note that the only difference between equations from right FPE and from left FPE from the paper is that right FPE equations has all positive signs.


Let us now consider the Euler-Poincar\'{e} attitude dynamics with linear feedback control. The Lie group of interest is $ G = \SO(3) \ltimes \rr^{3}$, where the space of angular momenta $ \rr^{3}$, an additive Lie group, is identified with $ \mathfrak{so}(3)$ via the skew-symmetric operations $ ( \cdot )^\vee$ and $ ( \cdot )^\wedge $. Let $ R$ be the attitude in matrix form, $ \ell$ be the body-frame angular momentum, and  $ \omega$ be the body-frame angular velocity, so $ g = (R, \ell)$ with $ \ell = \mathbb{I} \omega$. Next, we present the exponential map $ \exp:  \mathfrak{g} \supset U \to V \subset G$ of $ G$. We also denote the exponential map of $ \SO(3)$ as $ \exp ( \xi) := \expm \left( \xi^\wedge \right) $. The input should resolve any ambiguity. We claim that TODO
\begin{align*}
	\exp \begin{pmatrix} \xi \\ x \end{pmatrix} = \begin{pmatrix} \exp( \xi )\\ J_l( \xi) x \end{pmatrix} ,
\end{align*}
where
\begin{align*}
	J_l(x) := I + \frac{1- \cos \norm{ x} }{ \norm{ x}^2 } x^\wedge + \frac{\norm{ x} - \sin \norm{ x}  }{ \norm{ x}^3 } ( x^\wedge )^2 
\end{align*}
is the left Jacobian of the $ \SO(3)$ exponential map. Note that $ J_l(0) = I$.

Assume control has the form $ u = K \ell$ and shares the same channel as the noise with scaling matrix $ B$. Then the stochastic control process is modeled by the following Stratonovich SDE (Ito's is non-physical):
\begin{align*}
	g^{-1}\d g &= h^s(g)\d t+ H \textcircled{s}  \d W,  
\end{align*}
where
\begin{align*}
	h^s \begin{pmatrix} R\\ \ell \end{pmatrix} = \begin{pmatrix} \mathbb{I} ^{-1} \ell \\ - \ell^\wedge  \times \mathbb{I}^{-1} \ell + BK \ell \end{pmatrix}  , \qquad H = \begin{pmatrix} 0 &0\\0& B \end{pmatrix} .
\end{align*}

To obtain the 2nd-order approximation propagation, we need to compute the Jacobian and Hessian of following function:
\begin{align*}
	h^c \begin{pmatrix} \xi \\ x \end{pmatrix} &:= h^s \left( g \exp \begin{pmatrix} \xi \\ x \end{pmatrix}  \right) \\
						   & = h^s \begin{pmatrix} \overline{R} \exp(\xi) \\ \overline{ \ell} + J_l(\xi)x \end{pmatrix}  \\
						   &=  \\
\end{align*}




\begin{align*}
	D( x^\wedge I^{-1} x) e_j = \left(-\left( I^{-1} x \right)^\wedge  + x^\wedge I^{-1} \right) e_j
\end{align*}
Then
\begin{align*}
	\partial x_i \left(-\left( I^{-1} x \right)^\wedge  + x^\wedge I^{-1} \right) e_j &= \partial x_i \left( (-I^{-1} x)^\wedge e_j \right) + E_i^{\SO(3)} I^{-1} e_j \\
	&= \partial x_i \left( e_j^\wedge I^{-1}x \right) + E_i^{\SO(3)} I^{-1} e_j\\
	&= E_j^{\SO(3)} I^{-1} e_i +  E_i^{\SO(3)} I^{-1} e_j .
\end{align*}
Since the term is symmetric, we can combine them into one when summed and scaled by entries of a symmetric matrix.

\section{Tangent Bundle of A Lie Group}
\subsection{Group Theory}
We first review two key concepts from group theory: group actions and semidirect products.
\begin{defn}
	Let $ G$ be a group and let  $ A$ be a set. A \allbold{group action} $ \rho: G \to \aut (A), g \mapsto \rho_g$ is a group homomorphism. That is, each element $ g \in G$ corresponds to an automorphism of $ A$, which means $ \rho_g$ sends any element $ a$ to another element $ \rho_g(a)$ in $ A$ in an invertible way. There are two types of group actions, namely left and right actions. The former is denoted $ g.a = \rho_g(a)$ and satisfies $ (gh).a = g.(h.a)$, and the latter is denoted $ a .g = \rho_g(a)$ and satisfies $ a.(gh) = (a .g).h$.
\end{defn}
The idea of group action is that since group elements have inverses, compose associatively, and contain an identity element, we can use them to encode automorphisms of a set, which themselves forms a group and thus share the same behaviors. Thus, it is easy to check that $ (\rho_g)^{-1} = \rho_{g^{-1}}$. Since the action is only required to be a group homomorphism but not isomorphism, it is possible that multiple group elements encode the same automorphism.

If $ A = G$, then there are three natural actions of  $ G$ on itself: given any $ g$, we have left multiplication $ g.h = gh$, right multiplication $ h.g = hg$, and conjugation $ g.h = ghg^{-1}$.

In the context of a Lie group $ G$, left and right multiplications are smooth automorphisms of $ G$ that induces derivatives on the tangent spaces. If $ v_h \in T_hG $, then the induced derivative of left multiplication is $T_h L_g (v_h) $ and that of right multiplication is $ T_hR_g(v_h)$. Notice that we need to keep careful track of the base point. Together, they induce an action on the Lie algebra $ \mathfrak{g} := T_e G $ called the \allbold{adjoint action}. Given any $ \xi \in \mathfrak{g} $, the adjoint action of $ g$, $ \Ad_{ g}: \mathfrak{g} \to \mathfrak{g} $ is defined as 
\begin{align*}
	\Ad_{ g}( \xi) &:= T_{g^{-1}} L_{g} \circ  T_e R_{g^{-1}} (\xi), \\
	&:= T_g R_{g^{-1}} \circ  T_e L_g (\xi) .
\end{align*}
In a sense, the adjoint action is conjugation by the derivatives. Notice that the derivatives of left and right multiplications commute with each other, just as left and right multiplications themselves. We simply need to make sure the base point is correct.

We shall see later that the definition of $ \Ad$ can indeed be derived from differentiating group conjugation. For now, let us review semidirect products.
\begin{defn}
	Let $ (K,*_K), (H,*_H)$ be groups and suppose there exists a group action $ \rho: K \to \aut (H)$.  Then the \allbold{semidirect product} $K \ltimes_\rho H$ is the set $ K \times H$ with the identity element $ (e_K,e_H)$ and is endowed with the following the group operation $ *$ induced by the group action $ \rho$:
\begin{align*}
(k_1,h_1)*(k_2,h_2) = (k_1 *_K k_2, h_1 *_H \rho_{k_1}(h_2))
\end{align*}
Thus, $ (k,h)^{-1} = (k^{-1}, \rho_{k^{-1}}(h^{-1}))$ since $ (k,h)*(k^{-1},\rho_{k^{-1}}(h^{-1})) = (e_K, h *_H [\rho_k \circ \rho_{k^{-1}}(h^{-1})]) = (e_K, e_H)$, where we use the fact that $ \rho_{k^{-1}} = \rho_k^{-1}$.

Alternatively, we can define a different group operation $ *$ such that
\begin{align*}
	(k_1,h_1)*(k_2,h_2) = (k_1 *_K k_2, \rho_{k_2 ^{-1}}(h_1) *_H h_2).
\end{align*}
This is an equally valid definition yet is rarely mentioned in the literature. We will see that this definition is vital for our purpose so we haphazardly name it the \allbold{cosemidirect product} and denote it as $ K \ltimes_{\rho^{-1}}^* H$. Then, $ (k,h)^{-1} = (k^{-1}, \rho_{k}(h^{-1}))$ since
\begin{align*}
	(k,h)*(k^{-1}, \rho_{k} (h^{-1})) = (e_K, \rho_k(h) *_H \rho_k(h^{-1})) = (e_K, \rho_k (h *_H h^{-1})) = (e_K, e_H).
\end{align*}
\end{defn}
\begin{eg}
Consider the Lie groups $ (\SO(3), \text{matmul}) $ and $ (\rr^3,+)$. The 3D special Euclidean group is defined as $ \SE(3) := \SO(3) \ltimes _{\Ad} \rr^3$, where $ \Ad_{ R}( v) = Rv$. Then its group operation $ *$ is
\begin{align*}
	(R_1,v_1)*(R_2,v_2) = (R_1R_2, v_1 + \Ad_{ R_1}( v_2)) = (R_1R_2, v_1+ R_1 v_2). 
\end{align*}
Observe that we can embed $ \SE(3)$ into  $ \SL(4, \rr)$ and convert $ *$ to simply matrix multiplication:
 \begin{align*}
	 (R_1,v_1)*(R_2,v_2) \mapsto  \begin{pmatrix} R_1& v_1\\ 0 & 1 \end{pmatrix} \begin{pmatrix} R_2 & v_2\\ 0& 1 \end{pmatrix} = \begin{pmatrix} R_1R_2& v_1 + R_1v_2\\0&1 \end{pmatrix}  .
\end{align*}
\end{eg}
\begin{eg}
We define the 3D left special Euclidean group as $ \LSE(3) := \SO(3) \ltimes_{\Ad^{-1}}^* \rr^3$. Since $ R^{-1} = R^{T}$ in $ \SO(3)$, the group operation $ *$ is
 \begin{align*}
	(R_1,v_1)*(R_2,v_2) = (R_1R_2, \Ad_{ R_2^{T}}( v_1) + v_2) = (R_1R_2, R_2^{T} v_1 + v_2). 
\end{align*}
Since the group operation is different from that of $ \SE(3)$, its matrix embedding must be different as well. Consider the embedding $ (R,v) \mapsto \begin{pmatrix} R & 0\\v^{T}&1 \end{pmatrix} $, then $ *$ also becomes matrix multiplication:
\begin{align*}
	 (R_1,v_1)*(R_2,v_2) \mapsto  \begin{pmatrix} R_1& 0\\ v_1^{T} & 1 \end{pmatrix} \begin{pmatrix} R_2 & 0\\ v_2^{T}& 1 \end{pmatrix} = \begin{pmatrix} R_1R_2& 0\\ v_1^{T}R_2 + v_2^{T} &1 \end{pmatrix} \mapsto (R_1 R_2, R_2^{T}v_1 + v_2) .
\end{align*}
\end{eg}

\subsection{Lie Group Structure of Tangent Bundle}
The right trivialization results from this section can be found at \cite{MarsdenRatiu1999} Chapter 4.1 (note the right action semidirect product mentioned there is not the cosemidirect product we have here). TODO Engo 2001 mentions some results but did not cite nor elaborate. 

Let $ G$ be a Lie group of dimension $ n$. The goal of this section is to show that the tangent bundle $ TG$  has two group-structure-preserving trivializations (meaning we can parametrize the group by another group whose underlying manifold is the trivial vector bundle $ G\times \rr^{n}$), which we call \allbold{left and right trivializations}. First, notice that $ G$ acts on itself by left or right multiplication. The action induces an action on the fibers of $ TG$: $ g.(h,v) = (gh, T_h L_g(v))$ or $(h,v).g = (hg, T_hR_g(v))$.  This induced action allows us to move the base point of all tangent vectors of $ G$ to the identity, so that all tangent vectors in $ TG$ are translated to the Lie algebra $ \mathfrak{g}$. This way, we can define left and right trivialization maps as
\begin{align*}
	LT &: TG \to G \ltimes_{\Ad^{-1}}^*   \mathfrak{g}, (g,v) \mapsto (g, T_g L_{g^{-1}}(v)),\\
	RT &: TG \to G \ltimes_{\Ad}   \mathfrak{g}, (g,v) \mapsto (g, T_g R_{g^{-1}}(v)).
\end{align*}

We claim that the two maps are Lie group isomorphisms. That is, it is a diffeomorphism to the direct product manifold and respects the (co)semidrect product structure. The inverse is $ (g,u)^{-1} = \left( g^{-1}, - \Ad_{ g}( u) \right)$. To prove the claim, we shall use the inverses of left and right trivialization maps instead, for convenience.
\begin{defn}
The left translation and right translation maps on $ TG$ are respectively defined as
\begin{align*}
	TL&: G\ltimes_{ \Ad^{-1}}^*  \mathfrak{ g} \to TG,\ (g,\xi) \mapsto (g,T_eL_g (\xi)), \\
	TR&: G \ltimes _{\Ad} \mathfrak{g} \to TG, (g, \xi) \mapsto (g, T_eR_g (\xi)) .
\end{align*}
\end{defn}

\begin{prop} 
Let $ G$ be a Lie group and  $ \mathfrak{ g }$ be its Lie algebra. Then the left and right translation maps $ TL$ and  $ TR$ are Lie group isomorphisms.
\end{prop}

\begin{proof}
Recall that multiplication on $ TG$ is the differential of the multiplication on $ G$. That is, if $ \mu: G \times G \to G, (g,h) \mapsto gh$, then product rule gives
\begin{align*}
	d\mu : TG \times TG \to TG,\ ((g, \xi) , (h, \eta)) \mapsto (gh, T_hL_g (\eta) + T_g R_h (\xi)).
\end{align*}

The left translation map $ TL: G\ltimes_{ \Ad^{-1}} \mathfrak{ g} \to TG,\ (g,\xi) \mapsto (g,T_eL_g \xi)$ is smooth since $d\mu$ is smooth, the map $(g,\xi) \mapsto  ((e,\xi),(g,0))$ is smooth, and $ d\mu ((e,\xi),(g,0)) = (g, T_eL_g(\xi) )$ is a composition of smooth maps. It is a diffeomorphism since it has inverse $LT: (g, v) \mapsto (g,T_g L_{g^{-1}}(v))$. It remains to show that it is also a Lie group homomorphism.

Since $G$ is a multiplicative group whereas $\mathfrak{g}$ as a vector space is an additive group, we have
\begin{align*}
    TL((g,\xi)*(h,\eta)) &= TL((gh,\ \Ad_{h^{-1}}(\xi) + \eta))\\
    &=(gh, T_e L_{gh}( \Ad_{ h^{-1}}( \xi) + \eta))\\
    &= (gh, T_e L_{gh} ( T_{h}L_{h^{-1}} \circ T_e R_{h} (\xi) + \eta))\\
    &= (gh, T_h L_g \circ T_eR_h(\xi) + T_hL_g \circ T_eL_h(\eta) )\\
    &= (gh, T_hL_g \circ T_eL_h(\eta) + T_gR_h \circ T_eL_g(\xi)) && \text{LR commute}  \\
    &= d\mu [(g, T_e L_g(\xi)) ,\ (h,T_eL_h (\eta)) ]\\
    &= d\mu [TL(g,\xi) ,\ TL(h,\eta)].
\end{align*}
Thus, $TL$ is a Lie group homomorphism and therefore an isomorphism. The results for $TR$ exactly mirror those of $ TL$. For completeness, we show
\begin{align*}
    TR((g,\xi)*(h,\eta)) &= TR((gh,\ \xi + \Ad_g(\eta)))\\
    &=(gh, T_e R_{gh}(\xi + \Ad_g(\eta)))\\
    &= (gh, T_e R_{gh} (\xi+ T_{g^{-1}}L_g \circ T_e R_{g^{-1}} (\eta)))\\
    &= (gh, T_gR_h \circ T_e R_g(\xi) + T_gR_h \circ T_eL_g(\eta))\\
    &= (gh, T_gR_h ( T_eR_g(\xi)) + T_hL_g (T_eR_h (\eta))) &&\text{LR commute} \\
    &= d\mu [(g, T_e R_g(\xi)) ,\ (h,T_eR_h (\eta)) ]\\
    &= d\mu [TR(g,\xi) ,\ TR(h,\eta)].
\end{align*}
\end{proof}

It turns out that both left and right trivializations yield the same Lie algebra! TODO: why? intuition? Thus we denote the Lie algebra by $ \mathfrak{g} \ltimes \mathfrak{g} $, which we claim is endowed with the following Lie bracket:
\begin{align*}
	[(\xi, u), (\eta,v)] := \left( [\xi, \eta], [\xi,v ] - [\eta, u] \right) .
\end{align*}
Its \allbold{adjoint representation} $ \ad_{ (\xi,u)} := [(\xi,u), -] : \mathfrak{g} \ltimes \mathfrak{g} \to \mathfrak{g} \ltimes \mathfrak{g} $. 
Note that the notations $ \Ad, \ad, [ -, - ] $ can come from $ G$ or the semidirect product, and any ambiguity can be resolved by inspecting their inputs. Now we derive the bracket formula.

Loosely speaking, the adjoint action $ \Ad$ is the derivative of group conjugation action at identity, and adjoint representation $ \ad$ is the derivative of adjoint action at identity. We make this precise in the context of the tangent bundle of $ G$: for any $ (g,u) \in G \ltimes_{\Ad^{-1}}^* \mathfrak{g} $, its action on $ G \ltimes_{\Ad^{-1}}^*  \mathfrak{g} $ by conjugation is
\begin{align*}
	\phi_{(g,u)} : G \ltimes_{\Ad^{-1}}^* \mathfrak{g}  \to G \ltimes_{\Ad^{-1}}^* \mathfrak{g} , (h,v) \mapsto (g,u)*(h,v)*(g,u)^{-1}.
\end{align*}
Explicitly, we have
\begin{align*}
	\phi_{(g,u)} (h,v) &= (g,u)*(h,v)*\left( g^{-1}, - \Ad_{ g}( u) \right)  \\
	&= (gh, \Ad_{ h^{-1}}( u) + v) * \left( g^{-1}, - \Ad_{ g}( u) \right)  \\
	&= \left( ghg^{-1}, \Ad_{ g}( \Ad_{ h^{-1}}( u) + v) - \Ad_{ g}( u) \right)  .
\end{align*}
Let $ (\gamma, c)$ be a smooth curve on $ G \times  \mathfrak{g} $ such that $ \gamma(0) = e,\ \dot{ \gamma}(0) = \eta \in \mathfrak{g},\ c(0) = 0,\ \dot{c}(0) = v$. Then the derivative of $ \phi_{(g,u)}$ at $ (e,0)$, the adjoint action $ \Ad_{(g,u)} : \mathfrak{g} \ltimes  \mathfrak{g}  \to \mathfrak{g}\ltimes  \mathfrak{g} $ is given by
 \begin{align*}
	 \Ad_{ (g,u)}(\eta, v ) &= \frac{d}{dt}\big|_{t= 0} \phi_g ( \gamma(t), c(t)) \\
				&= \frac{d}{dt}\big|_{t= 0} \left( g \gamma(t) g^{-1}, \Ad_{ g}( \Ad_{ \gamma(t)^{-1}}( u) + c(t)) - \Ad_{ g}( u) \right)  \\ 
						     &= \left( \Ad_{ g}( \eta), \Ad_{ g}(- \ad_{\dot{ \gamma}(0)} (u) + \dot{c}(0) ) \right)  \\
						     &= \left( \Ad_{ g}( \eta), \Ad_{ g}( -[ \eta, u] + v) \right)  ,
\end{align*}
where we use the fact that $ \ad_{ \dot{ \gamma}(0)} := \frac{d}{dt}\big|_{t= 0} \Ad_{ \gamma(t)}$ and the following result: 
\begin{align*}
\Ad_{ \gamma(t)^{-1}} \circ  \Ad_{ \gamma(t)} &= \id  \\
\frac{d}{dt}\big|_{t= 0} \left( \Ad_{ \gamma(t)^{-1}} \circ  \Ad_{ \gamma(t)} \right) &= 0 \\
\frac{d}{dt}\big|_{t= 0} \Ad_{ \gamma(t)^{-1}} \circ  \Ad_{ \gamma(0)} + \Ad_{ \gamma(0)^{-1}} \circ \ad_{ \dot{ \gamma}(0)} &= 0 && \text{ chain rule}\\
\frac{d}{dt}\big|_{t= 0} \Ad_{ \gamma(t)^{-1}} \circ \Ad_{ e} + \Ad_{ e} \circ \ad_{\dot{ \gamma}(0)} &= 0 \\
\frac{d}{dt}\big|_{t= 0} \Ad_{ \gamma(t)^{-1}} &= - \ad_{ \dot{ \gamma}(0)}  && \Ad_{ e} = \id  . 
\end{align*}
Finally, let $ (\gamma, c)$ be another smooth curve on $ G \times  \mathfrak{g}$ such that $ \gamma(0) = e,\ \dot{ \gamma}(0) = \xi,\ c(0) =0,\ \dot{c}(0) = u$. Then the derivative of $ \Ad_{ ( \gamma(t), c(t))}$ at $ t=0$ is the adjoint representation $ \ad_{ (\xi, u)}: \mathfrak{g} \ltimes^*  \mathfrak{g} \to \mathfrak{g} \ltimes^*  \mathfrak{g} $:
\begin{align*}
	\ad_{ (\xi,u)}(\eta, v) &= \frac{d}{dt}\big|_{t= 0} \Ad_{ ( \gamma(t), c(t))}( \eta,v ) \\
				&= \frac{d}{dt}\big|_{t= 0} \left( \Ad_{ \gamma(t)}( \eta), \Ad_{ \gamma(t)}( -[\eta, c(t)] + v) \right)    \\
				&= \left( \ad_{ \xi}(\eta), \ad_{ \xi} (-[\eta,c(0)]) + \Ad_{ \gamma(0)}( - [\eta, \dot{c}(0)] ) + \ad_{ \xi}(v) \right)   \\
				&= ([ \xi, \eta], [\xi, v] - [\eta, u]) .
\end{align*}

We collect the above results into a proposition:
\begin{prop}
The left trivialization $ G \ltimes_{\Ad^{-1}}^* \mathfrak{g} $ of $ TG$ has
 \begin{align*}
	\Ad_{ (g,u)}(\eta, v ) &= \left( \Ad_{ g}( \eta), \Ad_{ g}(v-[ \eta, u]) \right),\\
	\ad_{ (\xi,u)}(\eta, v) &=  ([ \xi, \eta], [\xi, v] - [\eta, u]). 
\end{align*}
\end{prop}
\begin{remark}
The right trivialization $ G \ltimes_{\Ad} \mathfrak{g} $ instead has adjoint action
\begin{align*}
	\Ad_{ (g,u)}(\eta, v ) &= \left( \Ad_{ g}( \eta), \Ad_{ g}(v)-[ \Ad_{ g}( \eta), u]) \right).
\end{align*}
This is different from that of the left trivialization. However, the adjoint representation is exactly the same. Moreover, $ \SE(3)$ manifests this structure for $ \TSO(3)$ through matrix multiplication. However, body-frame dynamics demand left trivialization, so we are stuck with the cosemidirect product $ \LSE(3)$.
\end{remark}

\subsection{Demystifying $ \LSE(3)$}

Recall that any finite-dimensional Lie algebra is a finite-dimensional vector space and thus is isomorphic to $ \rr^{n}$ after choosing a basis, and we convert back and forth using $ x^\wedge = X \in \mathfrak{g} $ and $ X^\vee = x \in \rr^{n}$. 

The Lie algebra elements of $ \mathfrak{lse}(3) = \mathfrak{so}(3) \ltimes \mathfrak{so}(3) \cong \rr^3 \ltimes \rr^3$ have the following matrix embedding:
\begin{align*}
	(\xi, v) \mapsto \begin{pmatrix} \xi^\wedge & 0\\ v^{T}&0 \end{pmatrix} ,
\end{align*}
where $ \xi^\wedge $ is a skew-symmetric matrix in $ \mathfrak{so}(3) $.

Now we have a matrix representation, we can easily verify the $ \Ad$ and  $ \ad$ maps for $ \LSE(3)$ using matrix multiplication.
\begin{align*}
	\Ad_{ (R,u)}( \eta,v)  &= (R,u) * (\eta,v) * (R,u)^{-1}   \\
			       &\mapsto \begin{pmatrix} R&0\\ u^{T} &1 \end{pmatrix}\begin{pmatrix} \eta^\wedge & 0 \\ v^{T}&0 \end{pmatrix}   \begin{pmatrix} R^{T}& 0\\ - Ru^{T}R^{T} & 1 \end{pmatrix}  \\
			       &= \begin{pmatrix} R \eta^\wedge & 0\\ u^{T} \eta^\wedge + v^{T} & 0 \end{pmatrix} \begin{pmatrix} R^{T}&0\\- Ru^{T}R^{T}&1 \end{pmatrix}  \\
			       &= \begin{pmatrix} R \eta^\wedge R^{T} & 0 \\ (u^{T} \eta^\wedge + v^{T})R^{T}&0\end{pmatrix}  \\
			       &\mapsto  (R \eta^\wedge R^{-1}, R(v - \eta^\wedge u)) && (\eta^\wedge )^{T} = - \eta^\wedge \\
			       &\mapsto  ( \Ad_{ R}( \eta  ), \Ad_{ R}( v - [\eta,u])) .
\end{align*}
In the last step, we use the following facts: 1. adjoint action of matrix Lie group is literally conjugation, $ \Ad_{ R}( \Omega) = R\Omega R^{-1}$;  2. adjoint action of matrix Lie group on $ \mathfrak{g} $ identified as $ \rr^{n}$ is simply matrix-vector multiplication, $ \Ad_{ R}( \omega) = R \omega = R \omega^\wedge R^{-1} $.

The adjoint representation of matrix Lie group is just the commutator $ \ad_{ X}(Y) := [X,Y] = XY - YX$: 
\begin{align*}
	\ad_{ ( \xi, u)} (\eta,v) &\mapsto  \begin{pmatrix} \xi^\wedge & 0\\u^{T}&0 \end{pmatrix}  \begin{pmatrix}  \eta^\wedge &0\\v^{T}&0 \end{pmatrix} - \begin{pmatrix}  \eta^\wedge &0\\v^{T}&0 \end{pmatrix} \begin{pmatrix} \xi^\wedge & 0\\u^{T}&0 \end{pmatrix}\\
				  &= \begin{pmatrix} \xi^\wedge \eta^\wedge & 0\\ u^{T} \eta^\wedge &0 \end{pmatrix} -  \begin{pmatrix} \eta^\wedge \xi^\wedge & 0\\ v^{T} \xi^\wedge &0 \end{pmatrix}  \\
				  &= \begin{pmatrix} \xi^\wedge \eta^\wedge - \eta^\wedge \xi^\wedge &0 \\ \left( (\eta^\wedge)^{T}u - (\xi^\wedge )^{T} v  \right)^{T} &0 \end{pmatrix}  \\
				  &\mapsto  \left( [ \xi, \eta], \xi^\wedge v - \eta^\wedge u  \right) && \xi, \eta \text{ skew-symmetric}  \\
				  &= \left( [ \xi, \eta], [\xi,v] - [\eta,u] \right)  .
\end{align*}
The results match those from the general case!

\subsection{Preparation for Propagation}
Using this matrix representation of $ \LSE(3)$, we can apply the propagation formula.

A natural basis for $ \mathfrak{lse}(3) $ is
 \begin{align*}
	E_i = \begin{cases}
		\begin{pmatrix} E_i^{ \mathfrak{so}(3)}&0\\0&0 \end{pmatrix} & i = 1,2,3,\\
		\begin{pmatrix} 0& 0 \\ e_{i-3}^{T} & 0 \end{pmatrix} & i = 4,5,6 . 
	\end{cases}
\end{align*}
Under this basis, we can express the linear action $ \Ad_{ (R,u)} : \mathfrak{so}(3) \ltimes \mathfrak{so}(3) \to \mathfrak{so}(3) \ltimes \mathfrak{so}(3) $ as a $ 6\times 6$ matrix, whose $ j$th column can be computed by  $ [(R,u)^\wedge  E_j (R,u)^{-\wedge } ]^\vee$, yielding
\begin{align*}
	[ \Ad_{ (R,u)}] = \begin{pmatrix} R &0\\ Ru^\wedge &R \end{pmatrix} .
\end{align*}

Similarly, since $ \ad_{ (\xi,u)}: \mathfrak{so}(3) \ltimes \mathfrak{so}(3) \to \mathfrak{so}(3) \ltimes \mathfrak{so}(3) $ is linear, let $ [\ad_{ (\xi,u)}]$ denote the $ 6 \times 6$ matrix representation under this basis. Thus, its $ j$th column can be computed by $ [(\xi,u)^\wedge, E_j]^\vee$, yielding
\begin{align*}
	[ \ad_{ (\xi,u)}] = \begin{pmatrix} \xi^\wedge &0\\ u^\wedge & \xi^\wedge  \end{pmatrix} .
\end{align*}
We can readily verify that
\begin{align*}
	[ \ad_{ (\xi,u)}] \begin{pmatrix} \eta \\ v \end{pmatrix} = \begin{pmatrix} \xi^\wedge &0\\ u^\wedge & \xi^\wedge  \end{pmatrix}  \begin{pmatrix} \eta \\ v \end{pmatrix} = \begin{pmatrix} \xi^\wedge \eta \\ \xi^\wedge v + u^\wedge  \eta \end{pmatrix} = \begin{pmatrix}[] [\xi, \eta] \\ [\xi,v] - [\eta, u] \end{pmatrix}  = \ad_{ ( \xi,u)} (\eta,v) .
\end{align*}

From this, we obtain a key expression in the propagation formula.
\begin{align*}
	\ad_{ i} :=[ \ad_{ E_i}] = \begin{cases}
		\begin{pmatrix} E_i^{ \mathfrak{so}(3) } & 0\\ 0 & E_i^{ \mathfrak{so}(3) }  \end{pmatrix} & i = 1,2,3\\
		\begin{pmatrix} 0&0\\ e_{i-3}^{T} &0 \end{pmatrix} & i = 4,5,6 .
	\end{cases}
\end{align*}
Let $ x = \begin{pmatrix} \eta\\v \end{pmatrix} $. The exponential map $ \exp: \mathfrak{g} \ltimes \mathfrak{g} \to G \ltimes_{\Ad^{-1}}^* \mathfrak{g}  $  is
\begin{align*}
	\exp( x^\wedge ) = \begin{pmatrix} \exp( \eta^\wedge )\\  J_\ell(\eta)^{T} v \end{pmatrix} ,
\end{align*}
because
\begin{align*}
	\begin{pmatrix} \eta^\wedge &0\\ v^{T} & 0 \end{pmatrix}^{k} &= \begin{pmatrix} (\eta^\wedge)^{k} &0\\ v^{T} (\eta^\wedge )^{k-1} &0 \end{pmatrix} ,\\
	\exp \begin{pmatrix} \eta^\wedge &0 \\ v^{T} &0 \end{pmatrix} &= \sum_{ k= 0}^{\infty} \frac{1}{k!} \begin{pmatrix} \eta^\wedge &0\\ v^{T} & 0 \end{pmatrix}^{k} \\
					 &=  \begin{pmatrix} \sum_{ k= 0}^{\infty} \frac{1}{k!}  \eta^\wedge &0\\ v^{T} \sum_{ k= 0}^{\infty}\frac{1}{k!} (\eta^\wedge )^{k-1} & 0 \end{pmatrix} \\
					 &= \begin{pmatrix} \exp( \eta^\wedge ) &0\\ \left(  J_{\ell}( \eta)^{T} v  \right)^{T} & 0  \end{pmatrix}  \\
					 &= \begin{pmatrix} \exp( \eta^\wedge ) &0 \\ \left( J_r(\eta) v \right)^{T} &0  \end{pmatrix} && \text{ only in } \mathfrak{lse}(3) .
\end{align*}
See \cite{Barfoot Integral Forms in Matrix Lie Groups Eq 17}. Due to skew-symmetry, it is easy to see via Rodriguez's formula that $ J_r = J_\ell^{T}$ in $ \SO(3)$.

Let $ \mu = ( \overline{R}, \overline{ \omega}) $. The remaining terms in the propagation equations involve 1st-order and 2nd-order derivatives at zero of
\begin{align*}
	h^{c}(x) := h^s(\mu \exp( x^\wedge )) &= h^s \begin{pmatrix} \overline{R} \exp( \eta^\wedge ) \\ [\Ad_{ \exp( -\eta^\wedge )}] \overline{\omega} + J_r(\eta)v \end{pmatrix}  \\
					    &= h^s \begin{pmatrix} \overline{R} \exp( \eta^\wedge ) \\ \exp( -[\ad_{ \eta^\wedge }]) \overline{\omega} + J_r(\eta) v \end{pmatrix}  && \Ad_{ \exp( X)} = \exp( \ad_{ X}) \\
	&= h^s \begin{pmatrix} \overline{R} \exp( \eta^\wedge ) \\ \exp( - \eta^\wedge) \overline{\omega} + J_r(\eta) v \end{pmatrix}  && [\ad_{ X}] = X \in \mathfrak{so}(3)  \\
	&= \begin{pmatrix} \widetilde{ \omega}\\ \mathbb{I}^{-1} \left( BK \widetilde{ \omega}-  \widetilde{ \omega}^\wedge \mathbb{ I} \widetilde{ \omega} \right) \end{pmatrix}  ,
\end{align*}
where $ \widetilde{ \omega} := \exp( - \eta^\wedge) \overline{\omega} + J_r(\eta) v$ and $ h^s (R, \omega) = \begin{pmatrix} \omega \\ \mathbb{I} ^{-1} \left( BK \omega  - \omega^\wedge \mathbb{ I} \omega \right) \end{pmatrix} $ is just the attitude dynamics with linear feedback control $ u( \omega) = K \omega$ from the Stratonovich SDE  $ g^{-1} \d g = h^s \d t + H^s \textcircled{s} \d W$, where $ H^s = \begin{pmatrix} 0&0\\0&B \end{pmatrix} $. 

It is easier to find the 1st and 2nd order derivatives of $ \widetilde{ \omega}$ first:
\begin{align*}
	a_j := \frac{\partial \omega}{\partial x_j}\bigg|_{(0,0)} =\begin{cases}
		-  \exp( -\eta^\wedge )\big|_{0} E_j^{ \mathfrak{so}(3) } \overline{\omega} + \partial_j Jr(\eta) v\big|_{(0,0)} = - E_j^{ \mathfrak{so}(3) } \overline{\omega} & j = 1,2,3,\\
		\partial_j J_r(\eta)v\big|_{(0,0} + J_r(\eta)\big|_0 e_{j-3} = e_{j-3} & j=4,5,6.
	\end{cases}
\end{align*}
\begin{align*}
	c_{ij} := \frac{\partial^2 \omega}{\partial { x_i} \partial x_j}\bigg|_{(0,0}  = \begin{cases}
		\exp( -\eta^\wedge )\big|_0 E_i^{\mathfrak{so}(3) } E_j^{\mathfrak{so}(3) } \overline{\omega} = E_i^{\mathfrak{so}(3) } E_j^{\mathfrak{so}(3) } \overline{\omega} & i,j=1,2,3,\\
		\partial_j J_r(\eta)\big|_0 e_{i-3} = - \frac{1}{2} E_j^{\mathfrak{so}(3) } e_{i-3} & i=4,5,6,\quad j=1,2,3,\\
		c_{ji} & i = 1,2,3, \quad j = 4,5,6\\
		0 & i,j=4,5,6 .
	\end{cases}
\end{align*}
Note that we use the series expansion of $ J_r(\eta)= I - \frac{1}{2} \eta^\wedge + \cdots$ to obtain $ \partial_j J_r(\eta)  = -\frac{1}{2} E_j^{\mathfrak{so}(3) }$ for $ j=1,2,3$.

Denote $ h^c = (h_1^c, h_2^c)$, we can compute the composite derivatives blockwise:
\begin{align*}
	\frac{\partial h_1^c}{\partial x_j}\bigg|_{(0,0)} = a_j, \qquad \frac{\partial^2 h_1^c}{\partial { x_i} \partial x_j} \bigg|_{(0,0)} = c_{ij},  
\end{align*}
and
\begin{align*}
	\frac{\partial h_2^c}{\partial x_j}\bigg|_{(0,0)} &= \mathbb{I} ^{-1} \left( BK a_j - \left( \partial_j( \widetilde{ \omega})^\wedge \mathbb{I}  \widetilde{ \omega} + \widetilde{ \omega}^\wedge \mathbb{I} \partial_j \widetilde{ \omega} \right)\big|_{(0,0)}  \right)  \\ 
	&= \mathbb{I} ^{-1}\left( BKa_j - \left( a_j^\wedge \mathbb{I} \overline{\omega} + \overline{\omega}^\wedge \mathbb{I} a_j \right)  \right)  ,\\
	\frac{\partial^2 h_2^c}{\partial { x_i} \partial x_j} \bigg|_{(0,0)} &= \mathbb{I} ^{-1} \left( BKc_{ij} - \left( \partial_{ij}^2(\widetilde{ \omega})^\wedge \mathbb{I} \widetilde{ \omega} + \partial_j(\widetilde{ \omega})^\wedge \mathbb{I} \partial_i\widetilde{ \omega} + \partial_i(\widetilde{ \omega})^\wedge \mathbb{I} \partial_j \widetilde{ \omega} + \widetilde{ \omega}^\wedge \mathbb{I} \partial_{ij}^2 \widetilde{ \omega} \right)\big|_{(0,0)}  \right)  \\
	&= \mathbb{I} ^{-1} \left( BK c_{ij} - \left( c_{ij}^\wedge \mathbb{I} \overline{\omega} + a_j^\wedge \mathbb{I} a_i + a_i^\wedge \mathbb{I} a_j + \overline{\omega}^\wedge \mathbb{I} c_{ij} \right)  \right)  ,
\end{align*}
 where we use the fact that $ \widetilde{ w}|_{(0,0)} = \overline{\omega}$. Due to symmetry, further simplifications can be achieved when their sums are weighted by a symmetric matrix like the covariance. But we will leave that for later.

We now have all the ingredients for the 2nd order mean and covariance propagation on $ \LSE(3)$.

\section{Jacobian}
Since $ \exp: \mathfrak{g} \to G $, its derivative at $ X= x^\wedge  \in \mathfrak{g} $ is $ T_X \exp: \mathfrak{g} \to T_{\exp(X)}G$. Let $ Y= y^\wedge  \in \mathfrak{g} $. We can compute the derivative by
\begin{align*}
	T_X \exp( Y) &= \frac{d}{dt}\big|_{t= 0} \exp( X+tY) \\ 
	&= \frac{d}{dt}\big|_{t= 0} \lim_{ n \to \infty} \left( I + \frac{X+tY}{ n} \right)^{n}  \\
	&= \lim_{ n \to \infty} \frac{d}{dt}\big|_{t= 0} \left( I + \frac{X+tY}{ n} \right)^{n}  && \text{uniform convergence of derivatives}  \\
	&= \lim_{ n \to \infty} \sum_{ k= 0}^{ n} \left( I+ \frac{X}{n} \right)^{k} \frac{Y}{n} \left( I+\frac{X}{n} \right)^{n-k-1}  && \text{generalized Leibniz rule}  \\
	&= \int_{ 0}^{ 1} \exp( tX) Y \exp( (1-t)X)\ \d t && \text{known identity}  \\
	&= \int_{ 0}^{ 1} \exp( tX) Y \exp( -tX)\ \d t \exp( X)  \\
	&= \int_{ 0}^{ 1} \Ad_{ \exp( tX)}( Y)\ \d t \exp( X)  .
\end{align*}
But since we not interested in any tangent space other than the Lie algebra, we should right or left translate this derivative back to the Lie algebra, which results in left and right Jacobian, respectively. That is,
\begin{align*}
	J_\ell(x)[y] &:= \left(T_{\exp(X)} R_{\exp(X)^{-1}} \circ  T_X \exp( y^\wedge ) \right)^\vee\\
		  &= \left( T_X \exp( y^\wedge ) \exp( X)^{-1}\right)^\vee && \text{matrix Lie group}   \\
		  &= \left(  \int_{ 0}^{ 1} \Ad_{ \exp( tX)}( y^\wedge )\ \d t \exp( X) \exp( X)^{-1} \right)^\vee  \\
	&= \left(\int_{ 0}^{ 1} [\Ad_{ \exp( tX)}]\ \d t  \right) y \\
	&= \int_{ 0}^{ 1} \exp( [ \ad_{t X}])\ \d t \ y .
\end{align*}
Thus, we see that
\begin{align*}
	J_{\ell}(x) &= \int_{ 0}^{ 1} \exp( [t \ad_{ X}])\ \d t  \\
		    &= \int_{ 0}^{ 1} \sum_{ k= 0}^{\infty}\frac{t^{k}}{k! }[ \ad_{ X}]^{k} \d t \\
		    &= \sum_{ k= 0}^{\infty} \int_{ 0}^{ 1} \frac{t^{k}}{ k!} [ \ad_{ X}]^{k} \d t \\
		    &= \sum_{ k= 0}^{\infty} \frac{t^{k+1}}{ (k+1)!} [ \ad_{ X}]^{k}\bigg|_0^1 \\
		    &= \sum_{ k= 0}^{\infty} \frac{[ \ad_{ X}]^{k}}{(k+1)!} .
\end{align*}
Similarly, by a change of variable $ s=-t$ and left translation, we obtain that
\begin{align*}
	J_r(x) &= \int_{ 0}^{ 1} \exp( [ -t\ad_{X}])\ \d t \ = \sum_{ k= 0}^{\infty} \frac{[- \ad_{ X}]^{k}}{(k+1)!}  .
\end{align*}

\section{Intuition of Hamiltonian Mechanics}
A symplectic manifold has a conanical 2-form $ \omega$, which locally (in Darboux coordinates) acts as a symplectic bilinear form $J:= \begin{pmatrix} 0&I\\-I & 0 \end{pmatrix} $. Intuitively this ``turns the tangent coordinates 90 degrees". That is, horizontal direction becomes vertical and vice versa (not too different from $ \begin{pmatrix} 0&1\\-1&0 \end{pmatrix} $ if we think $ x$-coordinate as horizontal tangent coordinates and  $ y$-coordinate as vertical tangent coordinates). When this is applied to gradient vector field  of a function $ H$, since gradient goes perpendicular to the contour lines of $ H$, by turning 90 degrees now the vector field goes along the contour lines (level sets), which means that along the flow of the rotated vector field, $ H$ is conserved. This vector field is the Hamiltonian vector field $ X_H$ of  $ H$. Bloch Exercise 3.1.2 shows that in local Darboux coordinates, indeed we have $ X_H = J df^{T} = J \nabla f$. However, since there is no metric on the manifold, we cannot actually obtain gradient vector field of a function $ H$ nor rotate it. Yet we can still do this rotation to flow along the contour lines thanks to the 2-form. Instead of rotating the gradient vector field of $ H$, we rotate the differential $ dH$ contravariantly and get a vector field via $ \iota_{{X_H}} \omega = dH$.

Moreover, when the concept is applied to the cotangent bundle of a manifold, we have a canonical 1-form $ \theta$ and a canonical 2-form $ \omega := -d \theta$ that is symplectic. We can write this 2-form in local coordinates, and then compute the $ X_H$ in local coordinates as well for any $ H$. If we choose $ H$ to be the energy function, then the resulting coordinates are exactly Hamilton's equations, which give us the flows where energy is conserved. Notice that we do not need to use calculus of variations at all here. Geometry completely determines the equations of motion.

\begin{align*}
	\dot{z} &= X_H(z) \\
	\dot{f}(z) &= \left\langle df(z), X_H(z) \right\rangle \ \forall \ f\\
	\dot{f} &= \left\{ f,H \right\} \ \forall \ f
\end{align*}

\section{Dual Quaternions}

Consider $ G = \mathbb{ H}_u$ with scalar-first convention. Then $ \mathfrak{g} = T_1 \mathbb{ H}_u $ the set of pure quaternions, which we can identify with 3D vectors via $ ^\vee: \mathfrak{g} \to \rr^3, (0,v) \mapsto v$ and $ ^\wedge : \rr^3 \to \mathfrak{g},  v \mapsto (0,v)$. The exponential map can be defined using the infinite series $ \exp( v) = \sum_{ k= 0}^{\infty} \frac{(v^\wedge )^{k}}{ k!}$, which by applying the identity $ (v^\wedge)^2 = - \norm{ v}^2 $ simplifies to
\begin{align*}
	\exp( v) = \begin{pmatrix} \cos \norm{ v}\\ \frac{\sin \norm{ v} }{\norm{ v}  }v  \end{pmatrix} .
\end{align*}
We see that the injectivity radius remains $ \norm{ v} < \pi$, the same as the exponential map of $ \SO(3)$. Recall that $ \mathbb{ H}_u $ is a double-cover of $ \SO(3)$ with isomorphic Lie algebra $ \mathfrak{so}(3)$. Since their exponential maps have the same injectivity radius, in order for the exponential map of $ \mathbb{ H}_u $ to cover the entire $ S^3$ that wraps around $ \SO(3)$ twice within the same injectivity radius, the exponential map of $ \mathbb{ H}_u $ should go twice as fast as that of $ \SO(3)$. Therefore, in order to encode the same rotation, axis-angle-wise we need half the angle $ \norm{ v}$ in $ T_1 \mathbb{ H}_u $ than we do in $ \mathfrak{so}(3) $. Another way to see this is just deriving everything from $ \SU(2)$ which is isomorphic to  $ \mathbb{ H}_u $ as a Lie group.

As a result, for $ u^\wedge ,v^\wedge  \in \mathfrak{g} $, define pure quaternion multiplication to be the cross product $ u^\wedge v^\wedge := (u \times v)^\wedge$, we have
\begin{align*}
	\ad_{ u^\wedge }(v^\wedge ) = [u^\wedge ,v^\wedge ] = u^\wedge v^\wedge -v^\wedge u^\wedge  = \left(u \times v - v\times u   \right)^\wedge   = \left(2 u \times v \right)^\wedge = 2u^\wedge v^\wedge.
\end{align*}
Thus, we can write
\begin{align*}
	\ad_{ u^\wedge } = 2 u^\wedge .
\end{align*}
Indeed, we see that adjoint operator for pure quaternions goes twice as fast as well. 

Note that we omit explicitly writing out two types of multiplication for quaternions. A unit quaternion that encodes a rotation acts on other quaternions by the quaternion multiplication, but a pure quaternion that encodes a tangent vector in the Lie algebra can only act on other pure quaternions by the cross product. Thus, ambiguity can be resolved by inspecting the types of the objects.

Since the adjoint representation for $ \mathbb{ H}_u$ has the same form and only differ by a factor of 2, the left and right Jacobians and their inverses of $ \mathbb{ H}_u $ follow from the same infinite series as those from $ \SO(3)$ modulo a factor of 2. By following Barfoot 25, we can then derive the left and right Jacobians of the cosemidirect product  $ G \ltimes_{\Ad^{-1}}^* \mathfrak{g} $, which has the form
\begin{align*}
	J_{\ell} \begin{pmatrix} \xi \\ u \end{pmatrix} &= \begin{pmatrix} J_\ell(\xi) & 0\\  Q(\xi,u) & J_{\ell}(\xi)\end{pmatrix}  \\
	J_r \begin{pmatrix} \xi\\u \end{pmatrix} &= \begin{pmatrix} J_r(\xi)&0\\ Q(\xi,u)^{T}& J_r(\xi) \end{pmatrix}  .
\end{align*}
And then by Schur's complement, we can invert the block matrices and obtain
\begin{align*}
	J_{r}^{-1} \begin{pmatrix} \xi\\u \end{pmatrix} &= \begin{pmatrix} J_{r}^{-1} & -J_{r}^{-1}Q^{T} J_r^{-1} \\ 0 & J_r^{-1} \end{pmatrix}  .
\end{align*}
And likewise for $ J_\ell^{-1}$. 

There is no way to convert generic matrix-vector multiplication into quaternion multiplication, since quaternion multiplication can only encodes matrices related to rigid-body motion. Thus, by using $ \ad_{ i}^{T} =  - e_i^\wedge $, we obtain

\begin{align*}
	M_{ij}^{\mu} &= \frac{2^3}{48} e_i^\wedge  e_k^\wedge  \left( - BB^{T} e_j \right)^\wedge e_k^\wedge + \frac{2^3}{48} e_k^\wedge  \left( - BB^{T} e_j \right)^\wedge (- e_i^\wedge ) e_k^\wedge+ \frac{1}{2} \left( 2 e_i^\wedge \left(\frac{\partial h^c}{\partial x_j} \right)^\wedge  + \left(\frac{\partial^2 h^c}{\partial { x_i} \partial x_j}\right)^\wedge   \right)  .
\end{align*}

\section{PMP}

Here is the story: first, we know that the endpoint of optimal trajectory must be on the boundary of the attainable set at final time for the extended system, since the total cost has to be optimal which is part of the extended system. PMP tells us the necessary conditions for all trajectories that have endpoints on the boundary. Since all trajectories follow the same velocity vector field (that satisfies typical assumptions), they are completely determined by their endpoints. TODO

The idea is that instead of imposing velocity constraints on the cost function, we lift the velocity field to the natural setting of velocity field of functions, the cotangent bundle, and include cost as an extended velocity field there. This unifies the state dynamics and the cost function dynamics into the same space. The Hamiltonian lift gives us back the trajectory after projection, but along the fiber direction the trajectory simply traces out the (potentially time-varying) level sets of the maximized Hamiltonian since the velocity field is the Hamiltonian vector field. When the Hamiltonian is autonomous, then along the trajectory the Hamiltonian is conserved. In this case, since Hamiltonian is constant along the trajectory, maximality of Hamiltonian along the trajectory reduces to maximality of Hamiltonian at the end point. For the time-varying case, we can adjoin time to the state with speed $ 1$, yielding an autonomous extended system so the same intuition applies. This provides another perspective of why we can find a costate trajectory just by knowing that its endpoint is on the boundary. TODO

PMP, via its necessary conditions, reduces the infinite-dimensional trajectory optimization problem to the $ (n-1)$-dimensional endpoint optimization problem on the boundary, which is equivalent to studying the $ (n-1)$-dimensional costate equation $ \dot{ \lambda}_t = X_H ( \lambda_t)  $ since $ X_H $ is homogenous.


\end{document}
